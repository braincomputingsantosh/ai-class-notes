{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_L8MnPwKZj9s",
        "outputId": "f3ffafd2-0ec6-451b-fe35-17cab42b3ba9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Package installation complete!\n",
            "üéØ DATA COLLECTION FOR AI: PRACTICAL EXAMPLES\n",
            "============================================================\n",
            "Welcome to the comprehensive data collection tutorial!\n",
            "This script will guide you through different data collection strategies.\n",
            "\n",
            "\n",
            "============================================================\n",
            "üìä SECTION 1: UNDERSTANDING DATA TYPES\n",
            "============================================================\n",
            "\n",
            "üî¢ 1.1 NUMERICAL DATA EXAMPLES\n",
            "----------------------------------------\n",
            "üè• Healthcare Numerical Data:\n",
            "   patient_id  age  systolic_bp  diastolic_bp  heart_rate  cholesterol   bmi\n",
            "0           1   52          110            94          64          229  26.6\n",
            "1           2   42          110            77          94          206  23.1\n",
            "2           3   54          124            80          71          195  21.6\n",
            "3           4   67           81            65          59          187  28.1\n",
            "4           5   41           85            74          81          140  30.2\n",
            "5           6   41          108            81          57          171  29.7\n",
            "6           7   68           99            68          74          181  20.8\n",
            "7           8   56          126            83          48          242  23.5\n",
            "8           9   37          101            73          56          213  26.7\n",
            "9          10   53           91            77          74          129  29.9\n",
            "\n",
            "Average age: 51.1 years\n",
            "Average BMI: 26.0\n",
            "\n",
            "üí∞ Finance Numerical Data:\n",
            "   customer_id  annual_income  credit_score  monthly_spending  \\\n",
            "0            1          28578           736              2324   \n",
            "1            2          33096           853              2785   \n",
            "2            3          20885           696              3682   \n",
            "3            4          19968           856              2085   \n",
            "4            5          54516           438              1853   \n",
            "5            6          71547           782              2098   \n",
            "6            7          35031           708              3232   \n",
            "7            8          59979           670              2763   \n",
            "8            9          43513           709              2076   \n",
            "9           10          26303           501              2910   \n",
            "\n",
            "   savings_balance  loan_amount  \n",
            "0            11154        15040  \n",
            "1             3875        13123  \n",
            "2             8236         3677  \n",
            "3            11321        11634  \n",
            "4             1913        12258  \n",
            "5              582         8581  \n",
            "6             1293        13709  \n",
            "7             2785        18232  \n",
            "8             8519        30089  \n",
            "9             9856        16396  \n",
            "\n",
            "Average credit score: 695\n",
            "Average monthly spending: $2580.80\n",
            "\n",
            "üè∑Ô∏è 1.2 CATEGORICAL DATA EXAMPLES\n",
            "----------------------------------------\n",
            "üè• Healthcare Categorical Data:\n",
            "   patient_id blood_type      diagnosis  gender insurance_type\n",
            "0           1         A+        Healthy  Female       Medicare\n",
            "1           2        AB-         Asthma    Male       Medicaid\n",
            "2           3         A+        Healthy    Male       Medicaid\n",
            "3           4         A-        Healthy    Male       Medicaid\n",
            "4           5         B-       Diabetes  Female       Medicaid\n",
            "5           6         B-        Healthy  Female        Private\n",
            "6           7        AB-   Hypertension    Male       Medicaid\n",
            "7           8         O+   Hypertension  Female       Medicare\n",
            "8           9         A-  Heart Disease  Female        Private\n",
            "9          10         B+         Asthma  Female       Medicare\n",
            "\n",
            "Blood type distribution:\n",
            "blood_type\n",
            "A+     2\n",
            "AB-    2\n",
            "A-     2\n",
            "B-     2\n",
            "O+     1\n",
            "B+     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üí∞ Finance Categorical Data:\n",
            "   customer_id         card_type risk_level employment_status education_level\n",
            "0            1        MasterCard     Medium         Full-time          Master\n",
            "1            2        MasterCard        Low         Full-time        Bachelor\n",
            "2            3  American Express       High     Self-employed        Bachelor\n",
            "3            4  American Express       High         Part-time          Master\n",
            "4            5          Discover       High         Full-time          Master\n",
            "5            6          Discover     Medium     Self-employed             PhD\n",
            "6            7              Visa       High         Full-time             PhD\n",
            "7            8              Visa       High         Full-time        Bachelor\n",
            "8            9          Discover       High         Part-time             PhD\n",
            "9           10  American Express       High     Self-employed     High School\n",
            "\n",
            "Card type distribution:\n",
            "card_type\n",
            "American Express    3\n",
            "Discover            3\n",
            "MasterCard          2\n",
            "Visa                2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "üìù 1.3 TEXT DATA EXAMPLES\n",
            "----------------------------------------\n",
            "üè• Healthcare Text Data (Clinical Notes):\n",
            "Patient 1: Patient complains of chest pain after exercise, lasting 5-10 minutes\n",
            "Patient 2: Treatment with medication has significantly reduced symptoms\n",
            "Patient 3: Regular blood pressure monitoring shows improvement over 3 months\n",
            "Patient 4: Patient reports feeling more energetic since starting new diet\n",
            "Patient 5: Recommended follow-up appointment in 6 weeks to assess progress\n",
            "\n",
            "üí∞ Finance Text Data (Customer Reviews):\n",
            "Customer 1: Excellent customer service, quick loan approval process\n",
            "Customer 2: High fees make this credit card less attractive than competitors\n",
            "Customer 3: Mobile banking app is user-friendly and reliable\n",
            "Customer 4: Investment advisor provided helpful guidance on portfolio diversification\n",
            "Customer 5: Long wait times for customer support calls are frustrating\n",
            "\n",
            "üìà 1.4 TIME SERIES DATA EXAMPLES\n",
            "----------------------------------------\n",
            "üè• Healthcare Time Series (Daily Glucose Readings):\n",
            "        date  glucose_level\n",
            "0 2024-01-01           94.3\n",
            "1 2024-01-02          121.8\n",
            "2 2024-01-03          135.0\n",
            "3 2024-01-04          122.3\n",
            "4 2024-01-05          111.0\n",
            "5 2024-01-06          107.3\n",
            "6 2024-01-07          132.2\n",
            "7 2024-01-08          127.1\n",
            "8 2024-01-09          115.7\n",
            "9 2024-01-10          136.0\n",
            "\n",
            "üí∞ Finance Time Series (Daily Spending Pattern):\n",
            "        date  daily_spending\n",
            "0 2024-01-01          105.62\n",
            "1 2024-01-02          133.66\n",
            "2 2024-01-03           97.01\n",
            "3 2024-01-04           75.06\n",
            "4 2024-01-05           75.71\n",
            "5 2024-01-06           91.96\n",
            "6 2024-01-07          154.52\n",
            "7 2024-01-08           79.53\n",
            "8 2024-01-09           40.10\n",
            "9 2024-01-10           79.24\n",
            "\n",
            "‚úÖ Section 1 Complete: You now understand different data types!\n",
            "\n",
            "============================================================\n",
            "üóÇÔ∏è SECTION 2: WORKING WITH EXISTING DATASETS\n",
            "============================================================\n",
            "\n",
            "üìä 2.1 POPULAR EXISTING DATASETS\n",
            "----------------------------------------\n",
            "Available Datasets:\n",
            "                       name     domain                  size  quality                                         description\n",
            "MIMIC-III Clinical Database Healthcare      40,000+ patients       95 ICU patient data including vital signs and outcomes\n",
            "    NIH Chest X-ray Dataset Healthcare       100,000+ images       92     Chest X-ray images with disease classifications\n",
            "Credit Card Fraud Detection    Finance 280,000+ transactions       90          Credit card transactions with fraud labels\n",
            "   Yahoo Finance Stock Data    Finance  10+ years daily data       88         Historical stock prices and trading volumes\n",
            "\n",
            "üîç 2.2 DATASET EVALUATION TOOL\n",
            "----------------------------------------\n",
            "\n",
            "üî¨ Evaluating: High Quality Dataset\n",
            "------------------------------\n",
            "üìä Shape: 100 rows, 4 columns\n",
            "üìù Columns: feature_1, feature_2, feature_3, target\n",
            "\n",
            "‚ùå Missing Data Analysis:\n",
            "  ‚úÖ No missing values found!\n",
            "\n",
            "üìã Data Types:\n",
            "feature_1    float64\n",
            "feature_2    float64\n",
            "feature_3     object\n",
            "target         int64\n",
            "\n",
            "üéØ Quality Assessment:\n",
            "  Completeness: 100.0%\n",
            "  Duplicates: 0 (0.0%)\n",
            "  Overall Quality Score: 100.0/100\n",
            "\n",
            "üí° Recommendations:\n",
            "  ‚úÖ Excellent quality! Ready for AI training.\n",
            "\n",
            "üî¨ Evaluating: Low Quality Dataset\n",
            "------------------------------\n",
            "üìä Shape: 105 rows, 4 columns\n",
            "üìù Columns: feature_1, feature_2, feature_3, target\n",
            "\n",
            "‚ùå Missing Data Analysis:\n",
            "  feature_1: 16 (15.2%)\n",
            "  feature_2: 6 (5.7%)\n",
            "\n",
            "üìã Data Types:\n",
            "feature_1    float64\n",
            "feature_2    float64\n",
            "feature_3     object\n",
            "target         int64\n",
            "\n",
            "üéØ Quality Assessment:\n",
            "  Completeness: 94.8%\n",
            "  Duplicates: 5 (4.8%)\n",
            "  Overall Quality Score: 95.0/100\n",
            "\n",
            "üí° Recommendations:\n",
            "  ‚úÖ Excellent quality! Ready for AI training.\n",
            "\n",
            "‚úÖ Section 2 Complete: You can now evaluate existing datasets!\n",
            "\n",
            "============================================================\n",
            "ü§ñ SECTION 3: AUTOMATED DATA COLLECTION\n",
            "============================================================\n",
            "\n",
            "üï∑Ô∏è 3.1 WEB SCRAPING DEMONSTRATION\n",
            "----------------------------------------\n",
            "üîç Simulating web scraping of financial news...\n",
            "üì∞ Scraped Financial News Data:\n",
            "                                          headline  sentiment_score  timestamp\n",
            "0     Tech Stocks Rally on Strong Earnings Reports           -0.303 2024-01-01\n",
            "1  Federal Reserve Considers Interest Rate Changes            0.873 2024-01-02\n",
            "2         Healthcare Sector Shows Promising Growth           -0.922 2024-01-03\n",
            "3  Energy Prices Fluctuate Amid Market Uncertainty           -0.164 2024-01-04\n",
            "4     Banking Stocks Respond to Regulatory Updates            0.935 2024-01-05\n",
            "\n",
            "üìä Average sentiment: 0.084\n",
            "(Positive values indicate optimistic news, negative values indicate pessimistic news)\n",
            "\n",
            "üîå 3.2 API DATA COLLECTION\n",
            "----------------------------------------\n",
            "üì° Simulating API data collection...\n",
            "üìà Stock Market Data from API:\n",
            "        date  open_price  high_price  low_price  close_price   volume\n",
            "0 2024-01-01      150.00      150.76     149.59       150.16  1517313\n",
            "1 2024-01-02      152.78      153.89     151.80       153.62  2263108\n",
            "2 2024-01-03      156.52      157.38     156.42       156.96  3709532\n",
            "3 2024-01-04      159.90      160.69     159.63       160.14  4665060\n",
            "4 2024-01-05      158.67      159.30     157.32       158.69  4462565\n",
            "5 2024-01-06      159.43      161.69     156.07       159.46  2337682\n",
            "6 2024-01-07      160.42      162.87     159.29       159.70  2076871\n",
            "7 2024-01-08      157.36      157.44     155.22       157.17  2062383\n",
            "8 2024-01-09      157.72      158.31     157.38       158.16  4581292\n",
            "9 2024-01-10      159.86      160.16     159.61       159.75  3254656\n",
            "\n",
            "üìä Price range: $149.59 - $162.87\n",
            "üìä Average volume: 3,093,046 shares\n",
            "\n",
            "üì° 3.3 SENSOR DATA COLLECTION\n",
            "----------------------------------------\n",
            "üå°Ô∏è Simulating IoT sensor data collection...\n",
            "üè• Patient Monitoring Sensor Data:\n",
            "            timestamp  heart_rate  systolic_bp  temperature  oxygen_saturation\n",
            "0 2024-01-01 08:00:00          73           99         98.4               98.3\n",
            "1 2024-01-01 09:00:00          79          130         98.6               98.2\n",
            "2 2024-01-01 10:00:00          80          107         98.3               99.6\n",
            "3 2024-01-01 11:00:00          84          128         97.8               98.6\n",
            "4 2024-01-01 12:00:00          87          123         97.8               97.4\n",
            "5 2024-01-01 13:00:00          75          127         98.9               99.4\n",
            "6 2024-01-01 14:00:00          67          121         98.5               97.3\n",
            "7 2024-01-01 15:00:00          75          127         98.9               97.4\n",
            "8 2024-01-01 16:00:00          70          121         99.0               97.2\n",
            "9 2024-01-01 17:00:00          77          119         98.7               97.1\n",
            "\n",
            "üìä 24-hour averages:\n",
            "  Heart rate: 69.2 bpm\n",
            "  Blood pressure: 117.1 mmHg\n",
            "  Temperature: 98.6¬∞F\n",
            "  Oxygen saturation: 98.1%\n",
            "\n",
            "‚öñÔ∏è 3.4 AUTOMATION: BENEFITS vs CHALLENGES\n",
            "----------------------------------------\n",
            "üìä Automated vs Manual Collection Comparison (1-10 scale):\n",
            "            Aspect  Automated_Score  Manual_Score\n",
            "0            Speed                9             3\n",
            "1             Cost                8             4\n",
            "2      Consistency                9             5\n",
            "3      Scalability               10             3\n",
            "4  Quality Control                6             9\n",
            "5      Flexibility                4             9\n",
            "\n",
            "üí° Key Insights:\n",
            "‚úÖ Use automation for: High volume, consistent format, continuous data\n",
            "‚úÖ Use manual collection for: Quality control, complex judgments, unique contexts\n",
            "üéØ Best approach: Hybrid - combine both methods strategically\n",
            "\n",
            "‚úÖ Section 3 Complete: You understand automated data collection!\n",
            "\n",
            "============================================================\n",
            "üë• SECTION 4: MANUAL DATA COLLECTION\n",
            "============================================================\n",
            "\n",
            "üìã 4.1 SURVEY DESIGN FOR DATA COLLECTION\n",
            "----------------------------------------\n",
            "üè• Healthcare Survey Design Example:\n",
            "\n",
            "Question 1: What is your age?\n",
            "  Type: numerical\n",
            "  Validation: Must be between 18-120\n",
            "  Purpose: Demographic analysis and age-related health patterns\n",
            "\n",
            "Question 2: How would you rate your overall health?\n",
            "  Type: categorical\n",
            "  Options: Excellent, Very Good, Good, Fair, Poor\n",
            "  Purpose: Self-reported health status assessment\n",
            "\n",
            "Question 3: How many hours of sleep do you get per night on average?\n",
            "  Type: numerical\n",
            "  Validation: Must be between 1-24\n",
            "  Purpose: Sleep pattern analysis for health correlations\n",
            "\n",
            "Question 4: Describe any chronic health conditions you have:\n",
            "  Type: text\n",
            "  Purpose: Qualitative health information and condition tracking\n",
            "\n",
            "Question 5: How often do you exercise per week?\n",
            "  Type: categorical\n",
            "  Options: Never, 1-2 times, 3-4 times, 5+ times\n",
            "  Purpose: Activity level assessment for health predictions\n",
            "\n",
            "üìä 4.2 SIMULATED SURVEY RESPONSES\n",
            "----------------------------------------\n",
            "üìã Sample Survey Response Data:\n",
            "   respondent_id  age health_rating  sleep_hours exercise_frequency  \\\n",
            "0              1   56          Good     8.152492          1-2 times   \n",
            "1              2   36     Excellent     6.761484          1-2 times   \n",
            "2              3   53     Excellent     6.920857          3-4 times   \n",
            "3              4   55     Excellent     7.696998           5+ times   \n",
            "4              5   29          Good     4.573073              Never   \n",
            "\n",
            "               response_date  \n",
            "0 2025-06-13 23:45:00.644722  \n",
            "1 2025-06-18 23:45:00.644795  \n",
            "2 2025-06-09 23:45:00.644848  \n",
            "3 2025-07-08 23:45:00.644896  \n",
            "4 2025-06-11 23:45:00.644943  \n",
            "\n",
            "üìä Survey Analysis:\n",
            "  Total responses: 50\n",
            "  Average age: 49.4 years\n",
            "  Average sleep: 7.4 hours\n",
            "\n",
            "  Health rating distribution:\n",
            "health_rating\n",
            "Good         18\n",
            "Very Good    18\n",
            "Excellent    10\n",
            "Fair          3\n",
            "Poor          1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "‚úÖ Section 4 Complete: You understand manual data collection methods!\n",
            "\n",
            "============================================================\n",
            "üîç SECTION 5: DATA QUALITY ASSESSMENT\n",
            "============================================================\n",
            "\n",
            "üìä 5.1 DATA QUALITY METRICS\n",
            "----------------------------------------\n",
            "\n",
            "üî¨ Quality Assessment: High Quality Dataset\n",
            "------------------------------\n",
            "         Metric  Score      Status\n",
            "   Completeness  100.0 Excellent ‚úÖ\n",
            "     Uniqueness  100.0 Excellent ‚úÖ\n",
            "    Consistency   99.0 Excellent ‚úÖ\n",
            "Overall Quality   99.7 Excellent ‚úÖ\n",
            "\n",
            "üî¨ Quality Assessment: Low Quality Dataset\n",
            "------------------------------\n",
            "         Metric  Score      Status\n",
            "   Completeness   91.8     Good ‚ö†Ô∏è\n",
            "     Uniqueness   90.9     Good ‚ö†Ô∏è\n",
            "    Consistency   99.1 Excellent ‚úÖ\n",
            "Overall Quality   93.9     Good ‚ö†Ô∏è\n",
            "\n",
            "‚úÖ Section 5 Complete: You can now assess and improve data quality!\n",
            "\n",
            "============================================================\n",
            "üåü SECTION 6: REAL-WORLD CASE STUDIES\n",
            "============================================================\n",
            "\n",
            "üè• 6.1 HEALTHCARE CASE STUDY: PREDICTING PATIENT READMISSIONS\n",
            "------------------------------------------------------------\n",
            "üìã SCENARIO:\n",
            "A hospital wants to predict which patients are likely to be readmitted\n",
            "within 30 days of discharge to improve care coordination.\n",
            "\n",
            "‚úÖ Generated dataset with 1000 patients\n",
            "Sample data:\n",
            "   patient_id  age  gender insurance\n",
            "0           1   72    Male   Private\n",
            "1           2   62  Female  Medicare\n",
            "2           3   74    Male  Medicare\n",
            "3           4   87  Female  Medicare\n",
            "4           5   61    Male  Medicaid\n",
            "\n",
            "üí∞ 6.2 FINANCE CASE STUDY: FRAUD DETECTION SYSTEM\n",
            "------------------------------------------------------------\n",
            "üìã SCENARIO:\n",
            "A bank wants to build an AI system to detect fraudulent credit card\n",
            "transactions in real-time to protect customers.\n",
            "\n",
            "‚úÖ Generated 1,000 transactions\n",
            "Sample transaction data:\n",
            "   transaction_id  customer_id     amount merchant_category  hour_of_day\n",
            "0               1          103  58.392146           grocery           22\n",
            "1               2          180  13.012795        restaurant            9\n",
            "2               3           93   2.404150        restaurant            9\n",
            "3               4           15  49.062829        restaurant            1\n",
            "4               5          107   0.888082           grocery            8\n",
            "\n",
            "‚úÖ Section 6 Complete: You've seen real-world data collection in action!\n",
            "\n",
            "üéâ TUTORIAL COMPLETE!\n",
            "You've learned the fundamentals of data collection for AI!\n",
            "\n",
            "================================================================================\n",
            "üìã COMPREHENSIVE SUMMARY REPORT\n",
            "================================================================================\n",
            "                Section                                          Key Concepts\n",
            "          1. Data Types  Numerical, Categorical, Text, Time Series data types\n",
            "   2. Existing Datasets     Dataset evaluation, quality assessment, licensing\n",
            "3. Automated Collection      Web scraping, APIs, sensors, automation benefits\n",
            "   4. Manual Collection Surveys, interviews, expert annotation, human insight\n",
            "        5. Data Quality       Completeness, consistency, validation, cleaning\n",
            " 6. Real-World Examples  Healthcare readmission, fraud detection case studies\n",
            "\n",
            "üéØ KEY TAKEAWAYS FOR AI DATA COLLECTION:\n",
            "--------------------------------------------------\n",
            "  ‚úÖ Data quality is more important than data quantity\n",
            "  ‚úÖ Start with existing datasets when possible, supplement with custom collection\n",
            "  ‚úÖ Use automation for scale, manual collection for quality and nuance\n",
            "  ‚úÖ Always validate and clean your data before AI training\n",
            "  ‚úÖ Document everything - collection methods, quality issues, decisions made\n",
            "  ‚úÖ Consider legal and ethical implications from the beginning\n",
            "  ‚úÖ Implement quality control throughout the collection process\n",
            "  ‚úÖ Plan for data maintenance and updates over time\n",
            "\n",
            "================================================================================\n",
            "üéì CONGRATULATIONS!\n",
            "================================================================================\n",
            "You have completed the comprehensive data collection tutorial!\n",
            "You now have the knowledge and tools to:\n",
            "‚Ä¢ Choose the right data collection strategy for any AI project\n",
            "‚Ä¢ Implement quality control and validation procedures\n",
            "‚Ä¢ Balance automated and manual collection methods\n",
            "‚Ä¢ Apply these concepts in healthcare and finance domains\n",
            "\n",
            "üöÄ You're ready to start collecting data for your own AI projects!\n",
            "\n",
            "üìö Remember to check the references in the Deep Dive Document\n",
            "for additional resources and further learning opportunities!\n",
            "üìö DATA COLLECTION FOR AI - READY TO RUN!\n",
            "Execute main() to start the tutorial or run individual sections.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Data Collection for AI: Practical Examples\n",
        "==========================================\n",
        "\n",
        "This script demonstrates various data collection strategies and techniques\n",
        "for AI projects, designed for students without strong mathematical backgrounds.\n",
        "\n",
        "Topics covered:\n",
        "1. Understanding different data types and their characteristics\n",
        "2. Working with existing datasets vs. creating your own\n",
        "3. Automated vs. manual data collection methods\n",
        "4. Data quality assessment and validation\n",
        "5. Real-world examples in Healthcare and Finance domains\n",
        "\n",
        "Author: AI Education Assistant\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "# Install required packages (Google Colab compatible)\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_packages():\n",
        "    \"\"\"Install required packages with error handling\"\"\"\n",
        "    packages = ['requests', 'beautifulsoup4', 'pandas', 'numpy', 'matplotlib', 'seaborn']\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
        "        except subprocess.CalledProcessError:\n",
        "            print(f\"Warning: Could not install {package}\")\n",
        "\n",
        "    print(\"‚úÖ Package installation complete!\")\n",
        "\n",
        "# Run installation\n",
        "install_packages()\n",
        "\n",
        "# Import all necessary libraries with fallbacks\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "import random\n",
        "import string\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting style with fallback\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8')\n",
        "except:\n",
        "    try:\n",
        "        plt.style.use('seaborn')\n",
        "    except:\n",
        "        plt.style.use('default')\n",
        "\n",
        "try:\n",
        "    sns.set_palette(\"husl\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def generate_fake_name():\n",
        "    \"\"\"Generate fake names without faker library\"\"\"\n",
        "    first_names = ['John', 'Jane', 'Michael', 'Sarah', 'David', 'Emily', 'Chris', 'Lisa', 'Robert', 'Maria']\n",
        "    last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', 'Martinez']\n",
        "    return f\"{random.choice(first_names)} {random.choice(last_names)}\"\n",
        "\n",
        "def generate_fake_date(start_date='-5y', end_date='today'):\n",
        "    \"\"\"Generate fake dates without faker library\"\"\"\n",
        "    if start_date == '-5y':\n",
        "        start = datetime.now() - timedelta(days=5*365)\n",
        "    elif start_date == '-10y':\n",
        "        start = datetime.now() - timedelta(days=10*365)\n",
        "    elif start_date == '-1y':\n",
        "        start = datetime.now() - timedelta(days=365)\n",
        "    else:\n",
        "        start = datetime.strptime(start_date, '%Y-%m-%d')\n",
        "\n",
        "    if end_date == 'today':\n",
        "        end = datetime.now()\n",
        "    elif end_date == '-1y':\n",
        "        end = datetime.now() - timedelta(days=365)\n",
        "    else:\n",
        "        end = datetime.strptime(end_date, '%Y-%m-%d')\n",
        "\n",
        "    time_between = end - start\n",
        "    days_between = time_between.days\n",
        "    random_days = random.randrange(days_between)\n",
        "    return start + timedelta(days=random_days)\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function that runs all data collection examples\"\"\"\n",
        "\n",
        "    print(\"üéØ DATA COLLECTION FOR AI: PRACTICAL EXAMPLES\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Welcome to the comprehensive data collection tutorial!\")\n",
        "    print(\"This script will guide you through different data collection strategies.\\n\")\n",
        "\n",
        "    # Run all sections\n",
        "    section_1_data_types()\n",
        "    section_2_existing_datasets()\n",
        "    section_3_automated_collection()\n",
        "    section_4_manual_collection()\n",
        "    section_5_data_quality()\n",
        "    section_6_real_world_examples()\n",
        "\n",
        "    print(\"\\nüéâ TUTORIAL COMPLETE!\")\n",
        "    print(\"You've learned the fundamentals of data collection for AI!\")\n",
        "\n",
        "def section_1_data_types():\n",
        "    \"\"\"Section 1: Understanding Data Types with Examples\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üìä SECTION 1: UNDERSTANDING DATA TYPES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Set random seed for reproducible results\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # 1.1 NUMERICAL DATA EXAMPLES\n",
        "    print(\"\\nüî¢ 1.1 NUMERICAL DATA EXAMPLES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Healthcare numerical data\n",
        "    print(\"üè• Healthcare Numerical Data:\")\n",
        "    healthcare_numerical = {\n",
        "        'patient_id': range(1, 11),  # Smaller sample for display\n",
        "        'age': np.random.normal(45, 15, 10).astype(int),\n",
        "        'systolic_bp': np.random.normal(120, 20, 10).astype(int),\n",
        "        'diastolic_bp': np.random.normal(80, 10, 10).astype(int),\n",
        "        'heart_rate': np.random.normal(72, 12, 10).astype(int),\n",
        "        'cholesterol': np.random.normal(200, 40, 10).astype(int),\n",
        "        'bmi': np.random.normal(25, 5, 10).round(1)\n",
        "    }\n",
        "\n",
        "    healthcare_df = pd.DataFrame(healthcare_numerical)\n",
        "    print(healthcare_df)\n",
        "    print(f\"\\nAverage age: {healthcare_df['age'].mean():.1f} years\")\n",
        "    print(f\"Average BMI: {healthcare_df['bmi'].mean():.1f}\")\n",
        "\n",
        "    # Finance numerical data\n",
        "    print(\"\\nüí∞ Finance Numerical Data:\")\n",
        "    finance_numerical = {\n",
        "        'customer_id': range(1, 11),\n",
        "        'annual_income': np.random.lognormal(10.5, 0.5, 10).astype(int),\n",
        "        'credit_score': np.random.normal(700, 100, 10).astype(int),\n",
        "        'monthly_spending': np.random.normal(2500, 800, 10).astype(int),\n",
        "        'savings_balance': np.random.exponential(5000, 10).astype(int),\n",
        "        'loan_amount': np.random.normal(15000, 8000, 10).astype(int)\n",
        "    }\n",
        "\n",
        "    finance_df = pd.DataFrame(finance_numerical)\n",
        "    print(finance_df)\n",
        "    print(f\"\\nAverage credit score: {finance_df['credit_score'].mean():.0f}\")\n",
        "    print(f\"Average monthly spending: ${finance_df['monthly_spending'].mean():.2f}\")\n",
        "\n",
        "    # 1.2 CATEGORICAL DATA EXAMPLES\n",
        "    print(\"\\nüè∑Ô∏è 1.2 CATEGORICAL DATA EXAMPLES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Healthcare categorical data\n",
        "    print(\"üè• Healthcare Categorical Data:\")\n",
        "    blood_types = ['A+', 'A-', 'B+', 'B-', 'AB+', 'AB-', 'O+', 'O-']\n",
        "    diagnoses = ['Healthy', 'Hypertension', 'Diabetes', 'Heart Disease', 'Asthma']\n",
        "\n",
        "    healthcare_categorical = {\n",
        "        'patient_id': range(1, 11),\n",
        "        'blood_type': np.random.choice(blood_types, 10),\n",
        "        'diagnosis': np.random.choice(diagnoses, 10),\n",
        "        'gender': np.random.choice(['Male', 'Female'], 10),\n",
        "        'insurance_type': np.random.choice(['Private', 'Medicare', 'Medicaid'], 10)\n",
        "    }\n",
        "\n",
        "    healthcare_cat_df = pd.DataFrame(healthcare_categorical)\n",
        "    print(healthcare_cat_df)\n",
        "    print(f\"\\nBlood type distribution:\")\n",
        "    print(healthcare_cat_df['blood_type'].value_counts())\n",
        "\n",
        "    # Finance categorical data\n",
        "    print(\"\\nüí∞ Finance Categorical Data:\")\n",
        "    card_types = ['Visa', 'MasterCard', 'American Express', 'Discover']\n",
        "    risk_levels = ['Low', 'Medium', 'High']\n",
        "\n",
        "    finance_categorical = {\n",
        "        'customer_id': range(1, 11),\n",
        "        'card_type': np.random.choice(card_types, 10),\n",
        "        'risk_level': np.random.choice(risk_levels, 10),\n",
        "        'employment_status': np.random.choice(['Full-time', 'Part-time', 'Self-employed'], 10),\n",
        "        'education_level': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 10)\n",
        "    }\n",
        "\n",
        "    finance_cat_df = pd.DataFrame(finance_categorical)\n",
        "    print(finance_cat_df)\n",
        "    print(f\"\\nCard type distribution:\")\n",
        "    print(finance_cat_df['card_type'].value_counts())\n",
        "\n",
        "    # 1.3 TEXT DATA EXAMPLES\n",
        "    print(\"\\nüìù 1.3 TEXT DATA EXAMPLES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Healthcare text data\n",
        "    print(\"üè• Healthcare Text Data (Clinical Notes):\")\n",
        "    healthcare_text_samples = [\n",
        "        \"Patient complains of chest pain after exercise, lasting 5-10 minutes\",\n",
        "        \"Treatment with medication has significantly reduced symptoms\",\n",
        "        \"Regular blood pressure monitoring shows improvement over 3 months\",\n",
        "        \"Patient reports feeling more energetic since starting new diet\",\n",
        "        \"Recommended follow-up appointment in 6 weeks to assess progress\"\n",
        "    ]\n",
        "\n",
        "    for i, note in enumerate(healthcare_text_samples, 1):\n",
        "        print(f\"Patient {i}: {note}\")\n",
        "\n",
        "    # Finance text data\n",
        "    print(\"\\nüí∞ Finance Text Data (Customer Reviews):\")\n",
        "    finance_text_samples = [\n",
        "        \"Excellent customer service, quick loan approval process\",\n",
        "        \"High fees make this credit card less attractive than competitors\",\n",
        "        \"Mobile banking app is user-friendly and reliable\",\n",
        "        \"Investment advisor provided helpful guidance on portfolio diversification\",\n",
        "        \"Long wait times for customer support calls are frustrating\"\n",
        "    ]\n",
        "\n",
        "    for i, review in enumerate(finance_text_samples, 1):\n",
        "        print(f\"Customer {i}: {review}\")\n",
        "\n",
        "    # 1.4 TIME SERIES DATA EXAMPLES\n",
        "    print(\"\\nüìà 1.4 TIME SERIES DATA EXAMPLES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Create time series data\n",
        "    dates = pd.date_range(start='2024-01-01', end='2024-01-10', freq='D')\n",
        "\n",
        "    # Healthcare time series (blood glucose readings)\n",
        "    print(\"üè• Healthcare Time Series (Daily Glucose Readings):\")\n",
        "    base_glucose = 120\n",
        "    glucose_readings = base_glucose + np.random.normal(0, 15, len(dates))\n",
        "\n",
        "    healthcare_ts = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'glucose_level': glucose_readings.round(1)\n",
        "    })\n",
        "    print(healthcare_ts)\n",
        "\n",
        "    # Finance time series (daily spending)\n",
        "    print(\"\\nüí∞ Finance Time Series (Daily Spending Pattern):\")\n",
        "    daily_spending = []\n",
        "    for date in dates:\n",
        "        # Higher spending on weekends\n",
        "        if date.weekday() in [5, 6]:\n",
        "            base_spending = 150\n",
        "        else:\n",
        "            base_spending = 80\n",
        "        spending = base_spending + np.random.normal(0, 30)\n",
        "        daily_spending.append(max(0, spending))\n",
        "\n",
        "    finance_ts = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'daily_spending': np.array(daily_spending).round(2)\n",
        "    })\n",
        "    print(finance_ts)\n",
        "\n",
        "    print(\"\\n‚úÖ Section 1 Complete: You now understand different data types!\")\n",
        "\n",
        "def section_2_existing_datasets():\n",
        "    \"\"\"Section 2: Working with Existing Datasets\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üóÇÔ∏è SECTION 2: WORKING WITH EXISTING DATASETS\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 2.1 DATASET CATALOG\n",
        "    print(\"\\nüìä 2.1 POPULAR EXISTING DATASETS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Create a catalog of existing datasets\n",
        "    dataset_catalog = [\n",
        "        {\n",
        "            'name': 'MIMIC-III Clinical Database',\n",
        "            'domain': 'Healthcare',\n",
        "            'size': '40,000+ patients',\n",
        "            'quality': 95,\n",
        "            'description': 'ICU patient data including vital signs and outcomes'\n",
        "        },\n",
        "        {\n",
        "            'name': 'NIH Chest X-ray Dataset',\n",
        "            'domain': 'Healthcare',\n",
        "            'size': '100,000+ images',\n",
        "            'quality': 92,\n",
        "            'description': 'Chest X-ray images with disease classifications'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Credit Card Fraud Detection',\n",
        "            'domain': 'Finance',\n",
        "            'size': '280,000+ transactions',\n",
        "            'quality': 90,\n",
        "            'description': 'Credit card transactions with fraud labels'\n",
        "        },\n",
        "        {\n",
        "            'name': 'Yahoo Finance Stock Data',\n",
        "            'domain': 'Finance',\n",
        "            'size': '10+ years daily data',\n",
        "            'quality': 88,\n",
        "            'description': 'Historical stock prices and trading volumes'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    catalog_df = pd.DataFrame(dataset_catalog)\n",
        "    print(\"Available Datasets:\")\n",
        "    print(catalog_df.to_string(index=False))\n",
        "\n",
        "    # 2.2 DATASET EVALUATION FUNCTION\n",
        "    print(\"\\nüîç 2.2 DATASET EVALUATION TOOL\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def evaluate_dataset(data, dataset_name):\n",
        "        \"\"\"Comprehensive evaluation of dataset quality\"\"\"\n",
        "        print(f\"\\nüî¨ Evaluating: {dataset_name}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Basic info\n",
        "        print(f\"üìä Shape: {data.shape[0]} rows, {data.shape[1]} columns\")\n",
        "        print(f\"üìù Columns: {', '.join(data.columns.tolist())}\")\n",
        "\n",
        "        # Missing values\n",
        "        missing_data = data.isnull().sum()\n",
        "        missing_percent = (missing_data / len(data)) * 100\n",
        "\n",
        "        print(\"\\n‚ùå Missing Data Analysis:\")\n",
        "        if missing_data.sum() == 0:\n",
        "            print(\"  ‚úÖ No missing values found!\")\n",
        "        else:\n",
        "            for col in missing_data[missing_data > 0].index:\n",
        "                print(f\"  {col}: {missing_data[col]} ({missing_percent[col]:.1f}%)\")\n",
        "\n",
        "        # Data types\n",
        "        print(\"\\nüìã Data Types:\")\n",
        "        print(data.dtypes.to_string())\n",
        "\n",
        "        # Quality score calculation\n",
        "        completeness_score = (1 - missing_data.sum() / (len(data) * len(data.columns))) * 100\n",
        "        duplicates = data.duplicated().sum()\n",
        "        duplicate_percent = (duplicates / len(data)) * 100\n",
        "\n",
        "        quality_score = (completeness_score + (100 - duplicate_percent)) / 2\n",
        "\n",
        "        print(f\"\\nüéØ Quality Assessment:\")\n",
        "        print(f\"  Completeness: {completeness_score:.1f}%\")\n",
        "        print(f\"  Duplicates: {duplicates} ({duplicate_percent:.1f}%)\")\n",
        "        print(f\"  Overall Quality Score: {quality_score:.1f}/100\")\n",
        "\n",
        "        # Recommendations\n",
        "        print(\"\\nüí° Recommendations:\")\n",
        "        if quality_score >= 90:\n",
        "            print(\"  ‚úÖ Excellent quality! Ready for AI training.\")\n",
        "        elif quality_score >= 70:\n",
        "            print(\"  ‚ö†Ô∏è Good quality, minor cleaning needed.\")\n",
        "        else:\n",
        "            print(\"  ‚ùå Significant quality issues, extensive cleaning required.\")\n",
        "\n",
        "        return quality_score\n",
        "\n",
        "    # Demonstrate dataset evaluation\n",
        "    # Create sample datasets with different quality levels\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # High quality dataset\n",
        "    high_quality_data = pd.DataFrame({\n",
        "        'feature_1': np.random.normal(100, 15, 100),\n",
        "        'feature_2': np.random.normal(50, 10, 100),\n",
        "        'feature_3': np.random.choice(['A', 'B', 'C'], 100),\n",
        "        'target': np.random.choice([0, 1], 100)\n",
        "    })\n",
        "\n",
        "    # Low quality dataset (with missing values and duplicates)\n",
        "    low_quality_data = high_quality_data.copy()\n",
        "    low_quality_data.loc[0:10, 'feature_1'] = np.nan  # Add missing values\n",
        "    low_quality_data.loc[15:20, 'feature_2'] = np.nan\n",
        "    low_quality_data = pd.concat([low_quality_data, low_quality_data.iloc[0:5]])  # Add duplicates\n",
        "\n",
        "    # Evaluate both datasets\n",
        "    high_score = evaluate_dataset(high_quality_data, \"High Quality Dataset\")\n",
        "    low_score = evaluate_dataset(low_quality_data, \"Low Quality Dataset\")\n",
        "\n",
        "    print(\"\\n‚úÖ Section 2 Complete: You can now evaluate existing datasets!\")\n",
        "\n",
        "def section_3_automated_collection():\n",
        "    \"\"\"Section 3: Automated Data Collection\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ü§ñ SECTION 3: AUTOMATED DATA COLLECTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 3.1 WEB SCRAPING SIMULATION\n",
        "    print(\"\\nüï∑Ô∏è 3.1 WEB SCRAPING DEMONSTRATION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def simulate_web_scraping():\n",
        "        \"\"\"Simulate web scraping (for educational purposes)\"\"\"\n",
        "        print(\"üîç Simulating web scraping of financial news...\")\n",
        "\n",
        "        # Simulate scraped financial news headlines\n",
        "        simulated_headlines = [\n",
        "            \"Tech Stocks Rally on Strong Earnings Reports\",\n",
        "            \"Federal Reserve Considers Interest Rate Changes\",\n",
        "            \"Healthcare Sector Shows Promising Growth\",\n",
        "            \"Energy Prices Fluctuate Amid Market Uncertainty\",\n",
        "            \"Banking Stocks Respond to Regulatory Updates\"\n",
        "        ]\n",
        "\n",
        "        # Simulate sentiment scores (in reality, you'd use NLP)\n",
        "        sentiment_scores = np.random.uniform(-1, 1, len(simulated_headlines))\n",
        "\n",
        "        scraped_data = pd.DataFrame({\n",
        "            'headline': simulated_headlines,\n",
        "            'sentiment_score': sentiment_scores.round(3),\n",
        "            'timestamp': pd.date_range('2024-01-01', periods=len(simulated_headlines), freq='D')\n",
        "        })\n",
        "\n",
        "        return scraped_data\n",
        "\n",
        "    # Demonstrate web scraping\n",
        "    news_data = simulate_web_scraping()\n",
        "    print(\"üì∞ Scraped Financial News Data:\")\n",
        "    print(news_data)\n",
        "\n",
        "    print(f\"\\nüìä Average sentiment: {news_data['sentiment_score'].mean():.3f}\")\n",
        "    print(\"(Positive values indicate optimistic news, negative values indicate pessimistic news)\")\n",
        "\n",
        "    # 3.2 API DATA COLLECTION SIMULATION\n",
        "    print(\"\\nüîå 3.2 API DATA COLLECTION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def simulate_api_data_collection():\n",
        "        \"\"\"Simulate collecting data from APIs\"\"\"\n",
        "        print(\"üì° Simulating API data collection...\")\n",
        "\n",
        "        # Simulate stock price data (like from Yahoo Finance API)\n",
        "        dates = pd.date_range('2024-01-01', '2024-01-10', freq='D')\n",
        "        base_price = 150\n",
        "\n",
        "        # Simulate realistic price movements\n",
        "        price_changes = np.random.normal(0, 2, len(dates))\n",
        "        prices = [base_price]\n",
        "\n",
        "        for change in price_changes[1:]:\n",
        "            new_price = prices[-1] + change\n",
        "            prices.append(max(new_price, 1))  # Ensure price stays positive\n",
        "\n",
        "        api_data = pd.DataFrame({\n",
        "            'date': dates,\n",
        "            'open_price': prices,\n",
        "            'high_price': [p + abs(np.random.normal(0, 1)) for p in prices],\n",
        "            'low_price': [p - abs(np.random.normal(0, 1)) for p in prices],\n",
        "            'close_price': [p + np.random.normal(0, 0.5) for p in prices],\n",
        "            'volume': np.random.randint(1000000, 5000000, len(dates))\n",
        "        })\n",
        "\n",
        "        return api_data\n",
        "\n",
        "    # Demonstrate API data collection\n",
        "    stock_data = simulate_api_data_collection()\n",
        "    print(\"üìà Stock Market Data from API:\")\n",
        "    print(stock_data.round(2))\n",
        "\n",
        "    print(f\"\\nüìä Price range: ${stock_data['low_price'].min():.2f} - ${stock_data['high_price'].max():.2f}\")\n",
        "    print(f\"üìä Average volume: {stock_data['volume'].mean():,.0f} shares\")\n",
        "\n",
        "    # 3.3 SENSOR DATA COLLECTION SIMULATION\n",
        "    print(\"\\nüì° 3.3 SENSOR DATA COLLECTION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def simulate_sensor_data():\n",
        "        \"\"\"Simulate IoT sensor data collection\"\"\"\n",
        "        print(\"üå°Ô∏è Simulating IoT sensor data collection...\")\n",
        "\n",
        "        # Simulate patient monitoring sensors\n",
        "        timestamps = pd.date_range('2024-01-01 08:00:00', periods=24, freq='H')\n",
        "\n",
        "        # Simulate realistic vital signs with circadian rhythm\n",
        "        hours = np.array([t.hour for t in timestamps])\n",
        "\n",
        "        # Heart rate varies with time of day\n",
        "        base_hr = 70 + 10 * np.sin(2 * np.pi * (hours - 6) / 24)\n",
        "        heart_rate = base_hr + np.random.normal(0, 5, len(timestamps))\n",
        "\n",
        "        # Blood pressure follows similar pattern\n",
        "        base_systolic = 120 + 5 * np.sin(2 * np.pi * (hours - 8) / 24)\n",
        "        systolic_bp = base_systolic + np.random.normal(0, 8, len(timestamps))\n",
        "\n",
        "        sensor_data = pd.DataFrame({\n",
        "            'timestamp': timestamps,\n",
        "            'heart_rate': heart_rate.round(0).astype(int),\n",
        "            'systolic_bp': systolic_bp.round(0).astype(int),\n",
        "            'temperature': np.random.normal(98.6, 0.5, len(timestamps)).round(1),\n",
        "            'oxygen_saturation': np.random.normal(98, 1, len(timestamps)).round(1)\n",
        "        })\n",
        "\n",
        "        return sensor_data\n",
        "\n",
        "    # Demonstrate sensor data collection\n",
        "    sensor_data = simulate_sensor_data()\n",
        "    print(\"üè• Patient Monitoring Sensor Data:\")\n",
        "    print(sensor_data.head(10))\n",
        "\n",
        "    print(f\"\\nüìä 24-hour averages:\")\n",
        "    print(f\"  Heart rate: {sensor_data['heart_rate'].mean():.1f} bpm\")\n",
        "    print(f\"  Blood pressure: {sensor_data['systolic_bp'].mean():.1f} mmHg\")\n",
        "    print(f\"  Temperature: {sensor_data['temperature'].mean():.1f}¬∞F\")\n",
        "    print(f\"  Oxygen saturation: {sensor_data['oxygen_saturation'].mean():.1f}%\")\n",
        "\n",
        "    # 3.4 AUTOMATION BENEFITS AND CHALLENGES\n",
        "    print(\"\\n‚öñÔ∏è 3.4 AUTOMATION: BENEFITS vs CHALLENGES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    automation_analysis = {\n",
        "        'Aspect': ['Speed', 'Cost', 'Consistency', 'Scalability', 'Quality Control', 'Flexibility'],\n",
        "        'Automated_Score': [9, 8, 9, 10, 6, 4],\n",
        "        'Manual_Score': [3, 4, 5, 3, 9, 9]\n",
        "    }\n",
        "\n",
        "    comparison_df = pd.DataFrame(automation_analysis)\n",
        "    print(\"üìä Automated vs Manual Collection Comparison (1-10 scale):\")\n",
        "    print(comparison_df)\n",
        "\n",
        "    print(\"\\nüí° Key Insights:\")\n",
        "    print(\"‚úÖ Use automation for: High volume, consistent format, continuous data\")\n",
        "    print(\"‚úÖ Use manual collection for: Quality control, complex judgments, unique contexts\")\n",
        "    print(\"üéØ Best approach: Hybrid - combine both methods strategically\")\n",
        "\n",
        "    print(\"\\n‚úÖ Section 3 Complete: You understand automated data collection!\")\n",
        "\n",
        "def section_4_manual_collection():\n",
        "    \"\"\"Section 4: Manual Data Collection Methods\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üë• SECTION 4: MANUAL DATA COLLECTION\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 4.1 SURVEY DESIGN EXAMPLE\n",
        "    print(\"\\nüìã 4.1 SURVEY DESIGN FOR DATA COLLECTION\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def create_sample_survey():\n",
        "        \"\"\"Design a sample survey for healthcare data collection\"\"\"\n",
        "\n",
        "        healthcare_survey_questions = [\n",
        "            {\n",
        "                'question': 'What is your age?',\n",
        "                'type': 'numerical',\n",
        "                'validation': 'Must be between 18-120',\n",
        "                'purpose': 'Demographic analysis and age-related health patterns'\n",
        "            },\n",
        "            {\n",
        "                'question': 'How would you rate your overall health?',\n",
        "                'type': 'categorical',\n",
        "                'options': ['Excellent', 'Very Good', 'Good', 'Fair', 'Poor'],\n",
        "                'purpose': 'Self-reported health status assessment'\n",
        "            },\n",
        "            {\n",
        "                'question': 'How many hours of sleep do you get per night on average?',\n",
        "                'type': 'numerical',\n",
        "                'validation': 'Must be between 1-24',\n",
        "                'purpose': 'Sleep pattern analysis for health correlations'\n",
        "            },\n",
        "            {\n",
        "                'question': 'Describe any chronic health conditions you have:',\n",
        "                'type': 'text',\n",
        "                'purpose': 'Qualitative health information and condition tracking'\n",
        "            },\n",
        "            {\n",
        "                'question': 'How often do you exercise per week?',\n",
        "                'type': 'categorical',\n",
        "                'options': ['Never', '1-2 times', '3-4 times', '5+ times'],\n",
        "                'purpose': 'Activity level assessment for health predictions'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        return healthcare_survey_questions\n",
        "\n",
        "    survey_questions = create_sample_survey()\n",
        "    print(\"üè• Healthcare Survey Design Example:\")\n",
        "    for i, q in enumerate(survey_questions, 1):\n",
        "        print(f\"\\nQuestion {i}: {q['question']}\")\n",
        "        print(f\"  Type: {q['type']}\")\n",
        "        if 'options' in q:\n",
        "            print(f\"  Options: {', '.join(q['options'])}\")\n",
        "        if 'validation' in q:\n",
        "            print(f\"  Validation: {q['validation']}\")\n",
        "        print(f\"  Purpose: {q['purpose']}\")\n",
        "\n",
        "    # 4.2 SIMULATE SURVEY RESPONSES\n",
        "    print(\"\\nüìä 4.2 SIMULATED SURVEY RESPONSES\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def simulate_survey_responses(num_responses=50):\n",
        "        \"\"\"Simulate realistic survey responses\"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        responses = []\n",
        "        for i in range(num_responses):\n",
        "            response = {\n",
        "                'respondent_id': i + 1,\n",
        "                'age': np.random.randint(18, 80),\n",
        "                'health_rating': np.random.choice(['Excellent', 'Very Good', 'Good', 'Fair', 'Poor'],\n",
        "                                                p=[0.2, 0.3, 0.3, 0.15, 0.05]),\n",
        "                'sleep_hours': np.random.normal(7.5, 1.2),\n",
        "                'exercise_frequency': np.random.choice(['Never', '1-2 times', '3-4 times', '5+ times'],\n",
        "                                                     p=[0.2, 0.4, 0.3, 0.1]),\n",
        "                'response_date': datetime.now() - timedelta(days=np.random.randint(0, 30))\n",
        "            }\n",
        "            responses.append(response)\n",
        "\n",
        "        return pd.DataFrame(responses)\n",
        "\n",
        "    survey_data = simulate_survey_responses()\n",
        "    print(\"üìã Sample Survey Response Data:\")\n",
        "    print(survey_data.head())\n",
        "\n",
        "    # Analyze survey data\n",
        "    print(f\"\\nüìä Survey Analysis:\")\n",
        "    print(f\"  Total responses: {len(survey_data)}\")\n",
        "    print(f\"  Average age: {survey_data['age'].mean():.1f} years\")\n",
        "    print(f\"  Average sleep: {survey_data['sleep_hours'].mean():.1f} hours\")\n",
        "    print(f\"\\n  Health rating distribution:\")\n",
        "    print(survey_data['health_rating'].value_counts())\n",
        "\n",
        "    print(\"\\n‚úÖ Section 4 Complete: You understand manual data collection methods!\")\n",
        "\n",
        "def section_5_data_quality():\n",
        "    \"\"\"Section 5: Data Quality Assessment\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üîç SECTION 5: DATA QUALITY ASSESSMENT\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 5.1 QUALITY METRICS\n",
        "    print(\"\\nüìä 5.1 DATA QUALITY METRICS\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    def calculate_quality_metrics(data, dataset_name):\n",
        "        \"\"\"Calculate comprehensive quality metrics for a dataset\"\"\"\n",
        "\n",
        "        print(f\"\\nüî¨ Quality Assessment: {dataset_name}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        # Basic metrics\n",
        "        total_cells = data.shape[0] * data.shape[1]\n",
        "        missing_cells = data.isnull().sum().sum()\n",
        "        duplicate_rows = data.duplicated().sum()\n",
        "\n",
        "        # Calculate quality scores\n",
        "        completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
        "        uniqueness = ((data.shape[0] - duplicate_rows) / data.shape[0]) * 100\n",
        "\n",
        "        # Consistency check (for numerical columns)\n",
        "        numerical_cols = data.select_dtypes(include=[np.number]).columns\n",
        "        consistency_scores = []\n",
        "\n",
        "        for col in numerical_cols:\n",
        "            if len(data[col].dropna()) > 0:\n",
        "                # Check for outliers using IQR method\n",
        "                Q1 = data[col].quantile(0.25)\n",
        "                Q3 = data[col].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                outliers = data[(data[col] < Q1 - 1.5*IQR) | (data[col] > Q3 + 1.5*IQR)][col]\n",
        "                consistency = ((len(data[col]) - len(outliers)) / len(data[col])) * 100\n",
        "                consistency_scores.append(consistency)\n",
        "\n",
        "        avg_consistency = np.mean(consistency_scores) if consistency_scores else 100\n",
        "\n",
        "        # Overall quality score\n",
        "        overall_quality = (completeness + uniqueness + avg_consistency) / 3\n",
        "\n",
        "        # Display results\n",
        "        quality_metrics = {\n",
        "            'Metric': ['Completeness', 'Uniqueness', 'Consistency', 'Overall Quality'],\n",
        "            'Score': [completeness, uniqueness, avg_consistency, overall_quality],\n",
        "            'Status': []\n",
        "        }\n",
        "\n",
        "        # Assign status based on scores\n",
        "        for score in quality_metrics['Score']:\n",
        "            if score >= 95:\n",
        "                quality_metrics['Status'].append('Excellent ‚úÖ')\n",
        "            elif score >= 85:\n",
        "                quality_metrics['Status'].append('Good ‚ö†Ô∏è')\n",
        "            elif score >= 70:\n",
        "                quality_metrics['Status'].append('Fair ‚ö†Ô∏è')\n",
        "            else:\n",
        "                quality_metrics['Status'].append('Poor ‚ùå')\n",
        "\n",
        "        metrics_df = pd.DataFrame(quality_metrics)\n",
        "        metrics_df['Score'] = metrics_df['Score'].round(1)\n",
        "\n",
        "        print(metrics_df.to_string(index=False))\n",
        "\n",
        "        return overall_quality\n",
        "\n",
        "    # Create datasets with different quality levels for demonstration\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # High quality dataset\n",
        "    high_quality = pd.DataFrame({\n",
        "        'patient_id': range(1, 101),\n",
        "        'age': np.random.normal(50, 15, 100).astype(int),\n",
        "        'blood_pressure': np.random.normal(120, 20, 100).astype(int),\n",
        "        'cholesterol': np.random.normal(200, 40, 100).astype(int),\n",
        "        'diagnosis': np.random.choice(['Healthy', 'At Risk', 'Condition Present'], 100)\n",
        "    })\n",
        "\n",
        "    # Low quality dataset (many issues)\n",
        "    low_quality = high_quality.copy()\n",
        "    low_quality.loc[0:15, 'cholesterol'] = np.nan\n",
        "    low_quality.loc[20:30, 'blood_pressure'] = np.nan\n",
        "    low_quality.loc[5:8, 'age'] = np.nan\n",
        "    # Add duplicates\n",
        "    low_quality = pd.concat([low_quality, low_quality.iloc[0:10]])\n",
        "\n",
        "    # Assess datasets\n",
        "    high_score = calculate_quality_metrics(high_quality, \"High Quality Dataset\")\n",
        "    low_score = calculate_quality_metrics(low_quality, \"Low Quality Dataset\")\n",
        "\n",
        "    print(\"\\n‚úÖ Section 5 Complete: You can now assess and improve data quality!\")\n",
        "\n",
        "def section_6_real_world_examples():\n",
        "    \"\"\"Section 6: Real-World Case Studies\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üåü SECTION 6: REAL-WORLD CASE STUDIES\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 6.1 HEALTHCARE CASE STUDY\n",
        "    print(\"\\nüè• 6.1 HEALTHCARE CASE STUDY: PREDICTING PATIENT READMISSIONS\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(\"üìã SCENARIO:\")\n",
        "    print(\"A hospital wants to predict which patients are likely to be readmitted\")\n",
        "    print(\"within 30 days of discharge to improve care coordination.\\n\")\n",
        "\n",
        "    # Simulate comprehensive patient readmission dataset\n",
        "    np.random.seed(42)\n",
        "    n_patients = 1000\n",
        "\n",
        "    # Demographics (from EHR - high quality)\n",
        "    demographics = pd.DataFrame({\n",
        "        'patient_id': range(1, n_patients + 1),\n",
        "        'age': np.random.normal(65, 15, n_patients).astype(int),\n",
        "        'gender': np.random.choice(['Male', 'Female'], n_patients),\n",
        "        'insurance': np.random.choice(['Medicare', 'Private', 'Medicaid'], n_patients, p=[0.6, 0.3, 0.1])\n",
        "    })\n",
        "\n",
        "    print(f\"‚úÖ Generated dataset with {len(demographics)} patients\")\n",
        "    print(\"Sample data:\")\n",
        "    print(demographics.head())\n",
        "\n",
        "    # 6.2 FINANCE CASE STUDY\n",
        "    print(\"\\nüí∞ 6.2 FINANCE CASE STUDY: FRAUD DETECTION SYSTEM\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(\"üìã SCENARIO:\")\n",
        "    print(\"A bank wants to build an AI system to detect fraudulent credit card\")\n",
        "    print(\"transactions in real-time to protect customers.\\n\")\n",
        "\n",
        "    # Generate realistic transaction data\n",
        "    np.random.seed(42)\n",
        "    n_transactions = 1000\n",
        "\n",
        "    transactions = pd.DataFrame({\n",
        "        'transaction_id': range(1, n_transactions + 1),\n",
        "        'customer_id': np.random.randint(1, 200, n_transactions),\n",
        "        'amount': np.random.lognormal(3, 1.5, n_transactions),  # Log-normal for realistic amounts\n",
        "        'merchant_category': np.random.choice(['grocery', 'gas', 'restaurant', 'online', 'retail'],\n",
        "                                            n_transactions, p=[0.3, 0.15, 0.2, 0.25, 0.1]),\n",
        "        'hour_of_day': np.random.randint(0, 24, n_transactions),\n",
        "    })\n",
        "\n",
        "    print(f\"‚úÖ Generated {len(transactions):,} transactions\")\n",
        "    print(\"Sample transaction data:\")\n",
        "    print(transactions.head())\n",
        "\n",
        "    print(\"\\n‚úÖ Section 6 Complete: You've seen real-world data collection in action!\")\n",
        "\n",
        "def create_summary_report():\n",
        "    \"\"\"Create a comprehensive summary of all concepts covered\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"üìã COMPREHENSIVE SUMMARY REPORT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    summary_sections = {\n",
        "        'Section': [\n",
        "            '1. Data Types',\n",
        "            '2. Existing Datasets',\n",
        "            '3. Automated Collection',\n",
        "            '4. Manual Collection',\n",
        "            '5. Data Quality',\n",
        "            '6. Real-World Examples'\n",
        "        ],\n",
        "        'Key Concepts': [\n",
        "            'Numerical, Categorical, Text, Time Series data types',\n",
        "            'Dataset evaluation, quality assessment, licensing',\n",
        "            'Web scraping, APIs, sensors, automation benefits',\n",
        "            'Surveys, interviews, expert annotation, human insight',\n",
        "            'Completeness, consistency, validation, cleaning',\n",
        "            'Healthcare readmission, fraud detection case studies'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    summary_df = pd.DataFrame(summary_sections)\n",
        "    print(summary_df.to_string(index=False))\n",
        "\n",
        "    print(f\"\\nüéØ KEY TAKEAWAYS FOR AI DATA COLLECTION:\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    key_takeaways = [\n",
        "        \"‚úÖ Data quality is more important than data quantity\",\n",
        "        \"‚úÖ Start with existing datasets when possible, supplement with custom collection\",\n",
        "        \"‚úÖ Use automation for scale, manual collection for quality and nuance\",\n",
        "        \"‚úÖ Always validate and clean your data before AI training\",\n",
        "        \"‚úÖ Document everything - collection methods, quality issues, decisions made\",\n",
        "        \"‚úÖ Consider legal and ethical implications from the beginning\",\n",
        "        \"‚úÖ Implement quality control throughout the collection process\",\n",
        "        \"‚úÖ Plan for data maintenance and updates over time\"\n",
        "    ]\n",
        "\n",
        "    for takeaway in key_takeaways:\n",
        "        print(f\"  {takeaway}\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "        create_summary_report()\n",
        "\n",
        "        print(f\"\\n\" + \"=\"*80)\n",
        "        print(\"üéì CONGRATULATIONS!\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"You have completed the comprehensive data collection tutorial!\")\n",
        "        print(\"You now have the knowledge and tools to:\")\n",
        "        print(\"‚Ä¢ Choose the right data collection strategy for any AI project\")\n",
        "        print(\"‚Ä¢ Implement quality control and validation procedures\")\n",
        "        print(\"‚Ä¢ Balance automated and manual collection methods\")\n",
        "        print(\"‚Ä¢ Apply these concepts in healthcare and finance domains\")\n",
        "        print(\"\\nüöÄ You're ready to start collecting data for your own AI projects!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå An error occurred: {str(e)}\")\n",
        "        print(\"This is a learning script - errors are part of the learning process!\")\n",
        "        print(\"Try running individual sections to identify and fix issues.\")\n",
        "\n",
        "    finally:\n",
        "        print(f\"\\nüìö Remember to check the references in the Deep Dive Document\")\n",
        "        print(\"for additional resources and further learning opportunities!\")\n",
        "\n",
        "print(\"üìö DATA COLLECTION FOR AI - READY TO RUN!\")\n",
        "print(\"Execute main() to start the tutorial or run individual sections.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIJWPFN6ZpyE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}