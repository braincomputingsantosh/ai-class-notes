{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm6eOt4A-3jM"
      },
      "outputs": [],
      "source": [
        "# Data Conversion for AI: Practical Examples\n",
        "# Google Colab Notebook for hands-on learning\n",
        "\n",
        "\"\"\"\n",
        "This notebook provides practical examples of data conversion techniques for AI applications.\n",
        "It's designed to complement the deep dive document and help you understand the concepts\n",
        "through hands-on coding examples.\n",
        "\n",
        "Requirements:\n",
        "- No prior programming experience required\n",
        "- All examples are extensively commented\n",
        "- Run each cell in order for best results\n",
        "\n",
        "Table of Contents:\n",
        "1. Environment Setup\n",
        "2. Basic Data Type Conversions\n",
        "3. Text Data Conversion\n",
        "4. Image Data Conversion\n",
        "5. Time Series Data Conversion\n",
        "6. Healthcare Data Example\n",
        "7. Finance Data Example\n",
        "8. Best Practices Implementation\n",
        "\"\"\"\n",
        "\n",
        "# ========================================\n",
        "# 1. ENVIRONMENT SETUP\n",
        "# ========================================\n",
        "\n",
        "# Install required libraries (run this first in Google Colab)\n",
        "!pip install pandas numpy matplotlib seaborn scikit-learn pillow wordcloud textblob\n",
        "\n",
        "# Import all necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from PIL import Image\n",
        "import io\n",
        "import base64\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up visualization style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"✅ Environment setup complete!\")\n",
        "print(\"All libraries imported successfully\")\n",
        "\n",
        "# ========================================\n",
        "# 2. BASIC DATA TYPE CONVERSIONS\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"2. BASIC DATA TYPE CONVERSIONS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create sample data that mimics real-world messiness\n",
        "print(\"Creating sample 'messy' customer data...\")\n",
        "\n",
        "# Simulate raw customer data with various formats and issues\n",
        "raw_customer_data = {\n",
        "    'customer_id': ['C001', 'C002', 'C003', 'C004', 'C005'],\n",
        "    'name': ['John Smith', 'MARY JONES', 'bob wilson', 'Sarah Lee', 'Mike Brown'],\n",
        "    'age': ['25', '34 years', '45', 'thirty-two', '29'],\n",
        "    'income': ['$50,000', '65000', '$85,000.00', '45k', '70000'],\n",
        "    'height': ['5\\'10\"', '168 cm', '6 feet', '5.5 ft', '175'],\n",
        "    'date_joined': ['01/15/2020', '2021-03-22', 'March 1, 2019', '12-25-2021', '2020/07/30'],\n",
        "    'gender': ['M', 'Female', 'male', 'F', 'M'],\n",
        "    'satisfaction': ['Very Satisfied', 'Satisfied', 'Neutral', 'Dissatisfied', 'Very Satisfied']\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(raw_customer_data)\n",
        "print(\"\\nOriginal messy data:\")\n",
        "print(df)\n",
        "print(f\"\\nData types before conversion:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.1 Cleaning and Standardizing Names\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.1 Cleaning and standardizing names...\")\n",
        "\n",
        "def clean_names(name):\n",
        "    \"\"\"\n",
        "    Standardize name formatting to Title Case\n",
        "    \"\"\"\n",
        "    return name.strip().title()\n",
        "\n",
        "df['name_cleaned'] = df['name'].apply(clean_names)\n",
        "print(\"✅ Names standardized to Title Case\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.2 Converting Age Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.2 Converting messy age data to numbers...\")\n",
        "\n",
        "def extract_age(age_string):\n",
        "    \"\"\"\n",
        "    Extract numerical age from various text formats\n",
        "    \"\"\"\n",
        "    age_string = str(age_string).lower()\n",
        "\n",
        "    # Handle text numbers\n",
        "    text_to_num = {\n",
        "        'thirty-two': 32, 'thirty': 30, 'twenty': 20,\n",
        "        'forty': 40, 'fifty': 50, 'sixty': 60\n",
        "    }\n",
        "\n",
        "    if age_string in text_to_num:\n",
        "        return text_to_num[age_string]\n",
        "\n",
        "    # Extract numbers from strings like \"34 years\"\n",
        "    import re\n",
        "    numbers = re.findall(r'\\d+', age_string)\n",
        "    if numbers:\n",
        "        return int(numbers[0])\n",
        "\n",
        "    return None\n",
        "\n",
        "df['age_numeric'] = df['age'].apply(extract_age)\n",
        "print(\"✅ Ages converted to numeric format\")\n",
        "print(f\"Age conversion results: {df[['age', 'age_numeric']].to_dict('records')}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.3 Converting Income Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.3 Converting income data to standard format...\")\n",
        "\n",
        "def extract_income(income_string):\n",
        "    \"\"\"\n",
        "    Convert various income formats to numerical values\n",
        "    \"\"\"\n",
        "    income_string = str(income_string).lower()\n",
        "\n",
        "    # Remove currency symbols and commas\n",
        "    income_string = income_string.replace('$', '').replace(',', '')\n",
        "\n",
        "    # Handle 'k' suffix (thousands)\n",
        "    if 'k' in income_string:\n",
        "        number = float(income_string.replace('k', ''))\n",
        "        return number * 1000\n",
        "\n",
        "    # Extract pure numbers\n",
        "    import re\n",
        "    numbers = re.findall(r'[\\d.]+', income_string)\n",
        "    if numbers:\n",
        "        return float(numbers[0])\n",
        "\n",
        "    return None\n",
        "\n",
        "df['income_numeric'] = df['income'].apply(extract_income)\n",
        "print(\"✅ Income converted to numeric format\")\n",
        "print(f\"Income conversion results: {df[['income', 'income_numeric']].to_dict('records')}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.4 Converting Height Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.4 Converting height data to centimeters...\")\n",
        "\n",
        "def convert_height_to_cm(height_string):\n",
        "    \"\"\"\n",
        "    Convert various height formats to centimeters\n",
        "    \"\"\"\n",
        "    height_string = str(height_string).lower()\n",
        "\n",
        "    # Handle feet and inches (5'10\")\n",
        "    if \"'\" in height_string and '\"' in height_string:\n",
        "        import re\n",
        "        parts = re.findall(r\"(\\d+)'(\\d+)\", height_string)\n",
        "        if parts:\n",
        "            feet, inches = parts[0]\n",
        "            total_inches = int(feet) * 12 + int(inches)\n",
        "            return round(total_inches * 2.54, 1)  # Convert to cm\n",
        "\n",
        "    # Handle \"feet\" format\n",
        "    if 'feet' in height_string or 'ft' in height_string:\n",
        "        import re\n",
        "        numbers = re.findall(r'[\\d.]+', height_string)\n",
        "        if numbers:\n",
        "            feet = float(numbers[0])\n",
        "            total_inches = feet * 12\n",
        "            return round(total_inches * 2.54, 1)\n",
        "\n",
        "    # Handle centimeters\n",
        "    if 'cm' in height_string:\n",
        "        import re\n",
        "        numbers = re.findall(r'[\\d.]+', height_string)\n",
        "        if numbers:\n",
        "            return float(numbers[0])\n",
        "\n",
        "    # Handle plain numbers (assume cm if reasonable, feet if small)\n",
        "    import re\n",
        "    numbers = re.findall(r'[\\d.]+', height_string)\n",
        "    if numbers:\n",
        "        num = float(numbers[0])\n",
        "        if num < 10:  # Likely feet\n",
        "            return round(num * 12 * 2.54, 1)\n",
        "        else:  # Likely cm\n",
        "            return num\n",
        "\n",
        "    return None\n",
        "\n",
        "df['height_cm'] = df['height'].apply(convert_height_to_cm)\n",
        "print(\"✅ Heights converted to centimeters\")\n",
        "print(f\"Height conversion results: {df[['height', 'height_cm']].to_dict('records')}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.5 Converting Date Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.5 Converting various date formats...\")\n",
        "\n",
        "def standardize_date(date_string):\n",
        "    \"\"\"\n",
        "    Convert various date formats to standard YYYY-MM-DD\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Common date formats to try\n",
        "    formats = [\n",
        "        '%m/%d/%Y',    # 01/15/2020\n",
        "        '%Y-%m-%d',    # 2021-03-22\n",
        "        '%Y/%m/%d',    # 2020/07/30\n",
        "        '%m-%d-%Y',    # 12-25-2021\n",
        "        '%B %d, %Y'    # March 1, 2019\n",
        "    ]\n",
        "\n",
        "    for fmt in formats:\n",
        "        try:\n",
        "            date_obj = datetime.strptime(date_string, fmt)\n",
        "            return date_obj.strftime('%Y-%m-%d')\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "df['date_joined_standard'] = df['date_joined'].apply(standardize_date)\n",
        "print(\"✅ Dates standardized to YYYY-MM-DD format\")\n",
        "print(f\"Date conversion results: {df[['date_joined', 'date_joined_standard']].to_dict('records')}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.6 Converting Categorical Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.6 Converting categorical data using different encoding methods...\")\n",
        "\n",
        "# Gender encoding (binary)\n",
        "def standardize_gender(gender):\n",
        "    \"\"\"\n",
        "    Standardize gender representation\n",
        "    \"\"\"\n",
        "    gender = str(gender).lower().strip()\n",
        "    if gender in ['m', 'male']:\n",
        "        return 'Male'\n",
        "    elif gender in ['f', 'female']:\n",
        "        return 'Female'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['gender_standard'] = df['gender'].apply(standardize_gender)\n",
        "\n",
        "# One-hot encoding for gender\n",
        "gender_encoded = pd.get_dummies(df['gender_standard'], prefix='gender')\n",
        "df = pd.concat([df, gender_encoded], axis=1)\n",
        "\n",
        "print(\"✅ Gender data standardized and one-hot encoded\")\n",
        "\n",
        "# Ordinal encoding for satisfaction (has natural order)\n",
        "satisfaction_order = ['Dissatisfied', 'Neutral', 'Satisfied', 'Very Satisfied']\n",
        "satisfaction_mapping = {val: idx for idx, val in enumerate(satisfaction_order)}\n",
        "df['satisfaction_numeric'] = df['satisfaction'].map(satisfaction_mapping)\n",
        "\n",
        "print(\"✅ Satisfaction data converted to ordinal encoding\")\n",
        "print(f\"Satisfaction mapping: {satisfaction_mapping}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 2.7 Creating Derived Features\n",
        "# ----------------------------------------\n",
        "print(\"\\n2.7 Creating derived features...\")\n",
        "\n",
        "# Create age groups\n",
        "def categorize_age(age):\n",
        "    \"\"\"\n",
        "    Create age categories for analysis\n",
        "    \"\"\"\n",
        "    if age < 25:\n",
        "        return 'Young Adult'\n",
        "    elif age < 40:\n",
        "        return 'Adult'\n",
        "    elif age < 60:\n",
        "        return 'Middle Age'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "df['age_group'] = df['age_numeric'].apply(categorize_age)\n",
        "\n",
        "# Create income brackets\n",
        "def categorize_income(income):\n",
        "    \"\"\"\n",
        "    Create income brackets for analysis\n",
        "    \"\"\"\n",
        "    if income < 40000:\n",
        "        return 'Low Income'\n",
        "    elif income < 70000:\n",
        "        return 'Middle Income'\n",
        "    else:\n",
        "        return 'High Income'\n",
        "\n",
        "df['income_bracket'] = df['income_numeric'].apply(categorize_income)\n",
        "\n",
        "# Calculate BMI (if we had weight data, this is how we'd do it)\n",
        "# For demonstration, we'll use hypothetical weight\n",
        "np.random.seed(42)  # For reproducible results\n",
        "df['weight_kg'] = np.random.normal(70, 15, len(df))  # Simulated weight data\n",
        "df['bmi'] = df['weight_kg'] / ((df['height_cm'] / 100) ** 2)\n",
        "\n",
        "print(\"✅ Derived features created (age groups, income brackets, BMI)\")\n",
        "\n",
        "# Display final cleaned dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL CLEANED DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(df[['customer_id', 'name_cleaned', 'age_numeric', 'income_numeric',\n",
        "          'height_cm', 'date_joined_standard', 'gender_standard',\n",
        "          'satisfaction_numeric', 'age_group', 'income_bracket', 'bmi']].round(2))\n",
        "\n",
        "# ========================================\n",
        "# 3. TEXT DATA CONVERSION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"3. TEXT DATA CONVERSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create sample customer review data\n",
        "print(\"Creating sample customer review data...\")\n",
        "\n",
        "customer_reviews = [\n",
        "    \"This product is absolutely amazing! Great quality and fast delivery.\",\n",
        "    \"Terrible experience. The item broke after one day. Very disappointed.\",\n",
        "    \"Good value for money. Delivery was a bit slow but product is okay.\",\n",
        "    \"Excellent customer service. The team was very helpful and responsive.\",\n",
        "    \"Average product. Nothing special but does the job. Price is fair.\",\n",
        "    \"Outstanding quality! Exceeded my expectations. Highly recommend!\",\n",
        "    \"Poor packaging led to damaged item. Refund process was difficult.\",\n",
        "    \"Love this product! Using it daily and very satisfied with performance.\"\n",
        "]\n",
        "\n",
        "reviews_df = pd.DataFrame({\n",
        "    'review_id': range(1, len(customer_reviews) + 1),\n",
        "    'review_text': customer_reviews\n",
        "})\n",
        "\n",
        "print(f\"Created {len(customer_reviews)} sample reviews\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3.1 Basic Text Processing\n",
        "# ----------------------------------------\n",
        "print(\"\\n3.1 Basic text processing and cleaning...\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Clean and preprocess text data\n",
        "    \"\"\"\n",
        "    import re\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "reviews_df['review_cleaned'] = reviews_df['review_text'].apply(clean_text)\n",
        "print(\"✅ Text cleaned and preprocessed\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3.2 Tokenization (Breaking text into words)\n",
        "# ----------------------------------------\n",
        "print(\"\\n3.2 Tokenizing text into individual words...\")\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"\n",
        "    Split text into individual words (tokens)\n",
        "    \"\"\"\n",
        "    return text.split()\n",
        "\n",
        "reviews_df['tokens'] = reviews_df['review_cleaned'].apply(tokenize_text)\n",
        "reviews_df['word_count'] = reviews_df['tokens'].apply(len)\n",
        "\n",
        "print(\"✅ Text tokenized into words\")\n",
        "print(f\"Word count examples: {reviews_df[['review_text', 'word_count']].head(3).to_dict('records')}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3.3 Sentiment Analysis\n",
        "# ----------------------------------------\n",
        "print(\"\\n3.3 Converting text to sentiment scores...\")\n",
        "\n",
        "# Install textblob if not already installed\n",
        "try:\n",
        "    from textblob import TextBlob\n",
        "\n",
        "    def get_sentiment(text):\n",
        "        \"\"\"\n",
        "        Get sentiment polarity (-1 to 1) and subjectivity (0 to 1)\n",
        "        \"\"\"\n",
        "        blob = TextBlob(text)\n",
        "        return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
        "\n",
        "    # Apply sentiment analysis\n",
        "    sentiment_results = reviews_df['review_text'].apply(get_sentiment)\n",
        "    reviews_df['sentiment_polarity'] = [s[0] for s in sentiment_results]\n",
        "    reviews_df['sentiment_subjectivity'] = [s[1] for s in sentiment_results]\n",
        "\n",
        "    # Create sentiment categories\n",
        "    def categorize_sentiment(polarity):\n",
        "        if polarity > 0.1:\n",
        "            return 'Positive'\n",
        "        elif polarity < -0.1:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "    reviews_df['sentiment_category'] = reviews_df['sentiment_polarity'].apply(categorize_sentiment)\n",
        "\n",
        "    print(\"✅ Sentiment analysis completed\")\n",
        "    print(\"Sentiment distribution:\")\n",
        "    print(reviews_df['sentiment_category'].value_counts())\n",
        "\n",
        "except ImportError:\n",
        "    print(\"⚠️  TextBlob not available, using simple keyword-based sentiment\")\n",
        "\n",
        "    # Simple keyword-based sentiment analysis\n",
        "    positive_words = ['amazing', 'excellent', 'great', 'good', 'love', 'outstanding', 'recommend']\n",
        "    negative_words = ['terrible', 'poor', 'bad', 'disappointed', 'broke', 'difficult']\n",
        "\n",
        "    def simple_sentiment(text):\n",
        "        text_lower = text.lower()\n",
        "        pos_count = sum(1 for word in positive_words if word in text_lower)\n",
        "        neg_count = sum(1 for word in negative_words if word in text_lower)\n",
        "\n",
        "        if pos_count > neg_count:\n",
        "            return 'Positive'\n",
        "        elif neg_count > pos_count:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "\n",
        "    reviews_df['sentiment_category'] = reviews_df['review_text'].apply(simple_sentiment)\n",
        "    print(\"✅ Simple sentiment analysis completed\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3.4 Feature Extraction from Text\n",
        "# ----------------------------------------\n",
        "print(\"\\n3.4 Extracting numerical features from text...\")\n",
        "\n",
        "# Create additional text features\n",
        "def extract_text_features(text):\n",
        "    \"\"\"\n",
        "    Extract various numerical features from text\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Basic length features\n",
        "    features['char_count'] = len(text)\n",
        "    features['word_count'] = len(text.split())\n",
        "    features['avg_word_length'] = np.mean([len(word) for word in text.split()])\n",
        "\n",
        "    # Punctuation features\n",
        "    features['exclamation_count'] = text.count('!')\n",
        "    features['question_count'] = text.count('?')\n",
        "    features['capital_letters'] = sum(1 for c in text if c.isupper())\n",
        "\n",
        "    # Sentence features\n",
        "    features['sentence_count'] = len([s for s in text.split('.') if s.strip()])\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for each review\n",
        "text_features = reviews_df['review_text'].apply(extract_text_features)\n",
        "text_features_df = pd.DataFrame(list(text_features))\n",
        "\n",
        "# Combine with original data\n",
        "reviews_with_features = pd.concat([reviews_df, text_features_df], axis=1)\n",
        "\n",
        "print(\"✅ Text features extracted\")\n",
        "print(\"Sample text features:\")\n",
        "print(text_features_df.head())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 3.5 Converting Text to Numerical Vectors\n",
        "# ----------------------------------------\n",
        "print(\"\\n3.5 Converting text to numerical vectors using TF-IDF...\")\n",
        "\n",
        "# TF-IDF Vectorization (Term Frequency-Inverse Document Frequency)\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=20,  # Limit to top 20 words for simplicity\n",
        "    stop_words='english',  # Remove common words like 'the', 'and'\n",
        "    lowercase=True\n",
        ")\n",
        "\n",
        "# Fit and transform the text data\n",
        "tfidf_matrix = vectorizer.fit_transform(reviews_df['review_cleaned'])\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=[f'tfidf_{word}' for word in vectorizer.get_feature_names_out()]\n",
        ")\n",
        "\n",
        "print(\"✅ TF-IDF vectors created\")\n",
        "print(f\"Vector dimensions: {tfidf_matrix.shape}\")\n",
        "print(\"Top words identified:\")\n",
        "print(list(vectorizer.get_feature_names_out()))\n",
        "\n",
        "# Show final text conversion results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TEXT CONVERSION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(\"Features created from text:\")\n",
        "print(\"- Word count, character count, average word length\")\n",
        "print(\"- Punctuation counts (!, ?)\")\n",
        "print(\"- Sentiment analysis (positive/negative/neutral)\")\n",
        "print(\"- TF-IDF vectors for machine learning\")\n",
        "print(f\"- Total features created: {len(text_features_df.columns) + len(tfidf_df.columns)}\")\n",
        "\n",
        "# ========================================\n",
        "# 4. IMAGE DATA CONVERSION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"4. IMAGE DATA CONVERSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Since we can't upload actual images in this demo, we'll create synthetic image data\n",
        "print(\"Creating synthetic image data for demonstration...\")\n",
        "\n",
        "# Create a synthetic image (representing a simple pattern)\n",
        "def create_sample_image(size=(64, 64), pattern='gradient'):\n",
        "    \"\"\"\n",
        "    Create a sample image for demonstration\n",
        "    \"\"\"\n",
        "    if pattern == 'gradient':\n",
        "        # Create a gradient image\n",
        "        image = np.zeros(size)\n",
        "        for i in range(size[0]):\n",
        "            for j in range(size[1]):\n",
        "                image[i, j] = (i + j) / (size[0] + size[1]) * 255\n",
        "    elif pattern == 'checkerboard':\n",
        "        # Create a checkerboard pattern\n",
        "        image = np.zeros(size)\n",
        "        for i in range(size[0]):\n",
        "            for j in range(size[1]):\n",
        "                if (i // 8 + j // 8) % 2 == 0:\n",
        "                    image[i, j] = 255\n",
        "    else:\n",
        "        # Random noise\n",
        "        image = np.random.randint(0, 256, size)\n",
        "\n",
        "    return image.astype(np.uint8)\n",
        "\n",
        "# Create sample images\n",
        "sample_images = {\n",
        "    'gradient': create_sample_image(pattern='gradient'),\n",
        "    'checkerboard': create_sample_image(pattern='checkerboard'),\n",
        "    'noise': create_sample_image(pattern='noise')\n",
        "}\n",
        "\n",
        "print(\"✅ Sample images created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4.1 Image Preprocessing and Standardization\n",
        "# ----------------------------------------\n",
        "print(\"\\n4.1 Image preprocessing and standardization...\")\n",
        "\n",
        "def preprocess_image(image, target_size=(32, 32)):\n",
        "    \"\"\"\n",
        "    Preprocess image for AI processing\n",
        "    \"\"\"\n",
        "    from PIL import Image as PILImage\n",
        "\n",
        "    # Convert numpy array to PIL Image\n",
        "    pil_image = PILImage.fromarray(image)\n",
        "\n",
        "    # Resize to target size\n",
        "    resized = pil_image.resize(target_size)\n",
        "\n",
        "    # Convert back to numpy array\n",
        "    processed = np.array(resized)\n",
        "\n",
        "    # Normalize pixel values to 0-1 range\n",
        "    normalized = processed / 255.0\n",
        "\n",
        "    return normalized\n",
        "\n",
        "# Process all sample images\n",
        "processed_images = {}\n",
        "for name, image in sample_images.items():\n",
        "    processed_images[name] = preprocess_image(image)\n",
        "\n",
        "print(\"✅ Images preprocessed and normalized\")\n",
        "print(f\"Original size: {sample_images['gradient'].shape}\")\n",
        "print(f\"Processed size: {processed_images['gradient'].shape}\")\n",
        "print(f\"Pixel value range: {processed_images['gradient'].min():.2f} to {processed_images['gradient'].max():.2f}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4.2 Feature Extraction from Images\n",
        "# ----------------------------------------\n",
        "print(\"\\n4.2 Extracting numerical features from images...\")\n",
        "\n",
        "def extract_image_features(image):\n",
        "    \"\"\"\n",
        "    Extract statistical and structural features from an image\n",
        "    \"\"\"\n",
        "    features = {}\n",
        "\n",
        "    # Basic statistical features\n",
        "    features['mean_intensity'] = np.mean(image)\n",
        "    features['std_intensity'] = np.std(image)\n",
        "    features['min_intensity'] = np.min(image)\n",
        "    features['max_intensity'] = np.max(image)\n",
        "\n",
        "    # Histogram features (distribution of pixel intensities)\n",
        "    hist, _ = np.histogram(image.flatten(), bins=10, range=(0, 1))\n",
        "    for i, count in enumerate(hist):\n",
        "        features[f'histogram_bin_{i}'] = count\n",
        "\n",
        "    # Edge detection (simple gradient)\n",
        "    grad_x = np.gradient(image, axis=1)\n",
        "    grad_y = np.gradient(image, axis=0)\n",
        "    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n",
        "    features['edge_density'] = np.mean(gradient_magnitude)\n",
        "\n",
        "    # Contrast and texture measures\n",
        "    features['contrast'] = np.max(image) - np.min(image)\n",
        "    features['local_variance'] = np.mean(np.var(image.reshape(-1, 4), axis=1))\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features from all processed images\n",
        "image_features = {}\n",
        "for name, image in processed_images.items():\n",
        "    image_features[name] = extract_image_features(image)\n",
        "\n",
        "print(\"✅ Image features extracted\")\n",
        "print(\"Features extracted per image:\")\n",
        "feature_names = list(image_features['gradient'].keys())\n",
        "print(f\"- {len(feature_names)} total features\")\n",
        "print(f\"- Sample features: {feature_names[:5]}\")\n",
        "\n",
        "# Create DataFrame of image features\n",
        "image_features_df = pd.DataFrame(image_features).T\n",
        "print(\"\\nImage features summary:\")\n",
        "print(image_features_df.round(3))\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4.3 Converting Images to Pixel Arrays\n",
        "# ----------------------------------------\n",
        "print(\"\\n4.3 Converting images to pixel arrays for machine learning...\")\n",
        "\n",
        "def image_to_feature_vector(image):\n",
        "    \"\"\"\n",
        "    Flatten image to 1D array for machine learning\n",
        "    \"\"\"\n",
        "    return image.flatten()\n",
        "\n",
        "# Convert images to feature vectors\n",
        "image_vectors = {}\n",
        "for name, image in processed_images.items():\n",
        "    image_vectors[name] = image_to_feature_vector(image)\n",
        "\n",
        "print(\"✅ Images converted to feature vectors\")\n",
        "print(f\"Feature vector size: {len(image_vectors['gradient'])}\")\n",
        "print(f\"This means each image becomes {len(image_vectors['gradient'])} numerical features\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 4.4 Visualizing the Conversion Process\n",
        "# ----------------------------------------\n",
        "print(\"\\n4.4 Visualizing the image conversion process...\")\n",
        "\n",
        "# Create visualization of the conversion process\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Image Data Conversion Process', fontsize=16)\n",
        "\n",
        "# Show original images\n",
        "for idx, (name, image) in enumerate(sample_images.items()):\n",
        "    axes[0, idx].imshow(image, cmap='gray')\n",
        "    axes[0, idx].set_title(f'Original: {name.title()}')\n",
        "    axes[0, idx].axis('off')\n",
        "\n",
        "# Show processed images\n",
        "for idx, (name, image) in enumerate(processed_images.items()):\n",
        "    axes[1, idx].imshow(image, cmap='gray')\n",
        "    axes[1, idx].set_title(f'Processed: {name.title()}\\n{image.shape[0]}x{image.shape[1]} pixels')\n",
        "    axes[1, idx].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Visualization complete\")\n",
        "\n",
        "# ========================================\n",
        "# 5. TIME SERIES DATA CONVERSION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"5. TIME SERIES DATA CONVERSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create sample time series data (e.g., stock prices, sensor readings)\n",
        "print(\"Creating sample time series data...\")\n",
        "\n",
        "# Generate synthetic stock price data\n",
        "np.random.seed(42)\n",
        "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
        "base_price = 100\n",
        "price_changes = np.random.normal(0, 2, len(dates))\n",
        "prices = [base_price]\n",
        "\n",
        "for change in price_changes[1:]:\n",
        "    new_price = prices[-1] + change\n",
        "    prices.append(max(new_price, 10))  # Prevent negative prices\n",
        "\n",
        "stock_data = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'price': prices[:len(dates)],\n",
        "    'volume': np.random.randint(1000, 10000, len(dates))\n",
        "})\n",
        "\n",
        "print(f\"✅ Created {len(stock_data)} days of stock data\")\n",
        "print(\"Sample data:\")\n",
        "print(stock_data.head())\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5.1 Time-based Feature Engineering\n",
        "# ----------------------------------------\n",
        "print(\"\\n5.1 Creating time-based features...\")\n",
        "\n",
        "def extract_time_features(df):\n",
        "    \"\"\"\n",
        "    Extract various time-based features from datetime data\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Basic time features\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df['day_of_week'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "    df['day_of_year'] = df['date'].dt.dayofyear\n",
        "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
        "\n",
        "    # Cyclical features (important for capturing seasonal patterns)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "\n",
        "    # Business day features\n",
        "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
        "    df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
        "    df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
        "    df['is_quarter_start'] = df['date'].dt.is_quarter_start.astype(int)\n",
        "    df['is_quarter_end'] = df['date'].dt.is_quarter_end.astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "stock_data_with_time = extract_time_features(stock_data)\n",
        "print(\"✅ Time-based features created\")\n",
        "print(\"New time features:\")\n",
        "time_features = [col for col in stock_data_with_time.columns if col not in ['date', 'price', 'volume']]\n",
        "print(time_features)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5.2 Lag Features (Historical Values)\n",
        "# ----------------------------------------\n",
        "print(\"\\n5.2 Creating lag features (historical values)...\")\n",
        "\n",
        "def create_lag_features(df, column, lags=[1, 3, 7, 14, 30]):\n",
        "    \"\"\"\n",
        "    Create lag features for a given column\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for lag in lags:\n",
        "        df[f'{column}_lag_{lag}'] = df[column].shift(lag)\n",
        "\n",
        "    return df\n",
        "\n",
        "stock_data_with_lags = create_lag_features(stock_data_with_time, 'price')\n",
        "stock_data_with_lags = create_lag_features(stock_data_with_lags, 'volume')\n",
        "\n",
        "print(\"✅ Lag features created\")\n",
        "print(\"Lag features for price:\", [col for col in stock_data_with_lags.columns if 'price_lag' in col])\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5.3 Rolling Window Features\n",
        "# ----------------------------------------\n",
        "print(\"\\n5.3 Creating rolling window (moving average) features...\")\n",
        "\n",
        "def create_rolling_features(df, column, windows=[7, 14, 30]):\n",
        "    \"\"\"\n",
        "    Create rolling statistics features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for window in windows:\n",
        "        # Moving averages\n",
        "        df[f'{column}_ma_{window}'] = df[column].rolling(window=window).mean()\n",
        "\n",
        "        # Moving standard deviation (volatility)\n",
        "        df[f'{column}_std_{window}'] = df[column].rolling(window=window).std()\n",
        "\n",
        "        # Moving min and max\n",
        "        df[f'{column}_min_{window}'] = df[column].rolling(window=window).min()\n",
        "        df[f'{column}_max_{window}'] = df[column].rolling(window=window).max()\n",
        "\n",
        "        # Ratio to moving average (momentum indicator)\n",
        "        df[f'{column}_ratio_ma_{window}'] = df[column] / df[f'{column}_ma_{window}']\n",
        "\n",
        "    return df\n",
        "\n",
        "stock_data_with_rolling = create_rolling_features(stock_data_with_lags, 'price')\n",
        "print(\"✅ Rolling window features created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5.4 Change and Return Features\n",
        "# ----------------------------------------\n",
        "print(\"\\n5.4 Creating change and return features...\")\n",
        "\n",
        "def create_change_features(df, column):\n",
        "    \"\"\"\n",
        "    Create various change and return features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Absolute change\n",
        "    df[f'{column}_change'] = df[column].diff()\n",
        "\n",
        "    # Percentage change\n",
        "    df[f'{column}_pct_change'] = df[column].pct_change()\n",
        "\n",
        "    # Log returns (common in finance)\n",
        "    df[f'{column}_log_return'] = np.log(df[column] / df[column].shift(1))\n",
        "\n",
        "    # Cumulative changes\n",
        "    df[f'{column}_cumulative_return'] = (1 + df[f'{column}_pct_change']).cumprod() - 1\n",
        "\n",
        "    # Volatility (rolling standard deviation of returns)\n",
        "    df[f'{column}_volatility_7d'] = df[f'{column}_pct_change'].rolling(7).std()\n",
        "\n",
        "    return df\n",
        "\n",
        "stock_data_final = create_change_features(stock_data_with_rolling, 'price')\n",
        "print(\"✅ Change and return features created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 5.5 Technical Indicators\n",
        "# ----------------------------------------\n",
        "print(\"\\n5.5 Creating technical indicators...\")\n",
        "\n",
        "def create_technical_indicators(df):\n",
        "    \"\"\"\n",
        "    Create common technical indicators used in finance\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # RSI (Relative Strength Index) - simplified version\n",
        "    def calculate_rsi(prices, window=14):\n",
        "        delta = prices.diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "        return rsi\n",
        "\n",
        "    df['rsi'] = calculate_rsi(df['price'])\n",
        "\n",
        "    # MACD (Moving Average Convergence Divergence)\n",
        "    exp1 = df['price'].ewm(span=12).mean()\n",
        "    exp2 = df['price'].ewm(span=26).mean()\n",
        "    df['macd'] = exp1 - exp2\n",
        "    df['macd_signal'] = df['macd'].ewm(span=9).mean()\n",
        "\n",
        "    # Bollinger Bands\n",
        "    rolling_mean = df['price'].rolling(window=20).mean()\n",
        "    rolling_std = df['price'].rolling(window=20).std()\n",
        "    df['bollinger_upper'] = rolling_mean + (rolling_std * 2)\n",
        "    df['bollinger_lower'] = rolling_mean - (rolling_std * 2)\n",
        "    df['bollinger_position'] = (df['price'] - rolling_mean) / (2 * rolling_std)\n",
        "\n",
        "    return df\n",
        "\n",
        "stock_data_final = create_technical_indicators(stock_data_final)\n",
        "print(\"✅ Technical indicators created\")\n",
        "\n",
        "# Show summary of time series conversion\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TIME SERIES CONVERSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original features: 3 (date, price, volume)\")\n",
        "print(f\"Final features: {len(stock_data_final.columns)}\")\n",
        "print(f\"Features created: {len(stock_data_final.columns) - 3}\")\n",
        "\n",
        "print(\"\\nFeature categories created:\")\n",
        "print(\"- Time-based features (month, day, cyclical)\")\n",
        "print(\"- Lag features (historical values)\")\n",
        "print(\"- Rolling window features (moving averages, volatility)\")\n",
        "print(\"- Change features (returns, percentage changes)\")\n",
        "print(\"- Technical indicators (RSI, MACD, Bollinger Bands)\")\n",
        "\n",
        "# Display sample of final data\n",
        "print(\"\\nSample of converted time series data:\")\n",
        "sample_cols = ['date', 'price', 'price_change', 'price_ma_7', 'price_std_7', 'rsi', 'macd']\n",
        "print(stock_data_final[sample_cols].tail().round(3))\n",
        "\n",
        "# ========================================\n",
        "# 6. HEALTHCARE DATA EXAMPLE\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"6. HEALTHCARE DATA CONVERSION EXAMPLE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create synthetic patient data\n",
        "print(\"Creating synthetic patient data...\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_patients = 100\n",
        "\n",
        "# Generate realistic patient data\n",
        "patient_data = {\n",
        "    'patient_id': [f'P{i:04d}' for i in range(1, n_patients + 1)],\n",
        "    'age': np.random.randint(18, 85, n_patients),\n",
        "    'gender': np.random.choice(['Male', 'Female'], n_patients),\n",
        "    'height_cm': np.random.normal(170, 15, n_patients),\n",
        "    'weight_kg': np.random.normal(70, 15, n_patients),\n",
        "    'systolic_bp': np.random.normal(130, 20, n_patients),\n",
        "    'diastolic_bp': np.random.normal(80, 10, n_patients),\n",
        "    'heart_rate': np.random.normal(72, 12, n_patients),\n",
        "    'temperature': np.random.normal(98.6, 1.5, n_patients),\n",
        "    'blood_sugar': np.random.normal(100, 30, n_patients),\n",
        "    'cholesterol': np.random.normal(200, 40, n_patients),\n",
        "    'smoking_status': np.random.choice(['Never', 'Former', 'Current'], n_patients, p=[0.5, 0.3, 0.2]),\n",
        "    'family_history_diabetes': np.random.choice([0, 1], n_patients, p=[0.7, 0.3]),\n",
        "    'exercise_hours_week': np.random.exponential(3, n_patients),\n",
        "}\n",
        "\n",
        "patients_df = pd.DataFrame(patient_data)\n",
        "print(f\"✅ Created data for {n_patients} patients\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6.1 Medical Feature Engineering\n",
        "# ----------------------------------------\n",
        "print(\"\\n6.1 Creating medical risk indicators...\")\n",
        "\n",
        "def create_medical_features(df):\n",
        "    \"\"\"\n",
        "    Create medically relevant features from patient data\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # BMI calculation\n",
        "    df['bmi'] = df['weight_kg'] / ((df['height_cm'] / 100) ** 2)\n",
        "\n",
        "    # BMI categories\n",
        "    def categorize_bmi(bmi):\n",
        "        if bmi < 18.5:\n",
        "            return 'Underweight'\n",
        "        elif bmi < 25:\n",
        "            return 'Normal'\n",
        "        elif bmi < 30:\n",
        "            return 'Overweight'\n",
        "        else:\n",
        "            return 'Obese'\n",
        "\n",
        "    df['bmi_category'] = df['bmi'].apply(categorize_bmi)\n",
        "\n",
        "    # Blood pressure categories\n",
        "    def categorize_bp(systolic, diastolic):\n",
        "        if systolic < 120 and diastolic < 80:\n",
        "            return 'Normal'\n",
        "        elif systolic < 130 and diastolic < 80:\n",
        "            return 'Elevated'\n",
        "        elif systolic < 140 or diastolic < 90:\n",
        "            return 'High_Stage1'\n",
        "        else:\n",
        "            return 'High_Stage2'\n",
        "\n",
        "    df['bp_category'] = df.apply(lambda row: categorize_bp(row['systolic_bp'], row['diastolic_bp']), axis=1)\n",
        "\n",
        "    # Age risk categories\n",
        "    def categorize_age_risk(age):\n",
        "        if age < 40:\n",
        "            return 'Low'\n",
        "        elif age < 60:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'High'\n",
        "\n",
        "    df['age_risk'] = df['age'].apply(categorize_age_risk)\n",
        "\n",
        "    # Diabetes risk score (simplified)\n",
        "    def calculate_diabetes_risk(row):\n",
        "        risk_score = 0\n",
        "\n",
        "        # Age factor\n",
        "        if row['age'] > 45:\n",
        "            risk_score += 1\n",
        "\n",
        "        # BMI factor\n",
        "        if row['bmi'] > 25:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Blood pressure factor\n",
        "        if row['bp_category'] in ['High_Stage1', 'High_Stage2']:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Blood sugar factor\n",
        "        if row['blood_sugar'] > 100:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Family history\n",
        "        if row['family_history_diabetes'] == 1:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Exercise factor\n",
        "        if row['exercise_hours_week'] < 2:\n",
        "            risk_score += 1\n",
        "\n",
        "        # Smoking factor\n",
        "        if row['smoking_status'] == 'Current':\n",
        "            risk_score += 1\n",
        "\n",
        "        return risk_score\n",
        "\n",
        "    df['diabetes_risk_score'] = df.apply(calculate_diabetes_risk, axis=1)\n",
        "\n",
        "    # Cardiovascular risk indicators\n",
        "    df['hypertension'] = ((df['systolic_bp'] > 140) | (df['diastolic_bp'] > 90)).astype(int)\n",
        "    df['high_cholesterol'] = (df['cholesterol'] > 240).astype(int)\n",
        "    df['metabolic_syndrome_risk'] = (\n",
        "        (df['bmi'] > 30) &\n",
        "        (df['blood_sugar'] > 100) &\n",
        "        (df['systolic_bp'] > 130)\n",
        "    ).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "patients_with_features = create_medical_features(patients_df)\n",
        "print(\"✅ Medical features created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6.2 Encoding Categorical Medical Data\n",
        "# ----------------------------------------\n",
        "print(\"\\n6.2 Encoding categorical medical variables...\")\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "categorical_columns = ['gender', 'bmi_category', 'bp_category', 'age_risk', 'smoking_status']\n",
        "patients_encoded = pd.get_dummies(patients_with_features, columns=categorical_columns, prefix=categorical_columns)\n",
        "\n",
        "print(\"✅ Categorical variables encoded\")\n",
        "print(f\"Features before encoding: {len(patients_with_features.columns)}\")\n",
        "print(f\"Features after encoding: {len(patients_encoded.columns)}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6.3 Standardizing Medical Measurements\n",
        "# ----------------------------------------\n",
        "print(\"\\n6.3 Standardizing medical measurements...\")\n",
        "\n",
        "# Standardize continuous variables for machine learning\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "continuous_features = ['age', 'height_cm', 'weight_kg', 'bmi', 'systolic_bp',\n",
        "                      'diastolic_bp', 'heart_rate', 'temperature', 'blood_sugar',\n",
        "                      'cholesterol', 'exercise_hours_week']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "patients_scaled = patients_encoded.copy()\n",
        "patients_scaled[continuous_features] = scaler.fit_transform(patients_encoded[continuous_features])\n",
        "\n",
        "print(\"✅ Medical measurements standardized\")\n",
        "print(\"Standardization example (first 3 patients):\")\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Original BMI': patients_encoded['bmi'].head(3).round(2),\n",
        "    'Standardized BMI': patients_scaled['bmi'].head(3).round(2)\n",
        "})\n",
        "print(comparison_df)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 6.4 Creating Risk Prediction Features\n",
        "# ----------------------------------------\n",
        "print(\"\\n6.4 Creating comprehensive risk prediction features...\")\n",
        "\n",
        "# Create final feature set for risk prediction\n",
        "risk_features = patients_scaled.copy()\n",
        "\n",
        "# Add interaction features (combinations that might be medically significant)\n",
        "risk_features['age_bmi_interaction'] = risk_features['age'] * risk_features['bmi']\n",
        "risk_features['bp_age_interaction'] = risk_features['systolic_bp'] * risk_features['age']\n",
        "risk_features['cholesterol_bmi_interaction'] = risk_features['cholesterol'] * risk_features['bmi']\n",
        "\n",
        "print(\"✅ Risk prediction features created\")\n",
        "print(f\"Final feature count: {len(risk_features.columns)}\")\n",
        "\n",
        "# Display medical data conversion summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"HEALTHCARE DATA CONVERSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"Conversions performed:\")\n",
        "print(\"- BMI calculation and categorization\")\n",
        "print(\"- Blood pressure risk categorization\")\n",
        "print(\"- Diabetes risk scoring\")\n",
        "print(\"- Categorical variable encoding\")\n",
        "print(\"- Medical measurement standardization\")\n",
        "print(\"- Risk interaction features\")\n",
        "\n",
        "sample_medical_features = ['patient_id', 'bmi', 'bp_category', 'diabetes_risk_score',\n",
        "                          'hypertension', 'metabolic_syndrome_risk']\n",
        "print(\"\\nSample converted medical data:\")\n",
        "print(patients_with_features[sample_medical_features].head())\n",
        "\n",
        "# ========================================\n",
        "# 7. FINANCE DATA CONVERSION EXAMPLE\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"7. FINANCE DATA CONVERSION EXAMPLE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create synthetic financial transaction data\n",
        "print(\"Creating synthetic financial transaction data...\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_transactions = 1000\n",
        "\n",
        "# Generate realistic transaction data\n",
        "start_date = datetime(2023, 1, 1)\n",
        "end_date = datetime(2023, 12, 31)\n",
        "date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
        "\n",
        "transaction_data = {\n",
        "    'transaction_id': [f'TXN{i:06d}' for i in range(1, n_transactions + 1)],\n",
        "    'customer_id': [f'CUST{np.random.randint(1, 1001):04d}' for _ in range(n_transactions)],\n",
        "    'timestamp': np.random.choice(date_range, n_transactions),\n",
        "    'amount': np.random.lognormal(mean=4, sigma=1.5, size=n_transactions),  # Log-normal for realistic amounts\n",
        "    'merchant_category': np.random.choice(['Grocery', 'Gas', 'Restaurant', 'Retail', 'Entertainment', 'Healthcare'], n_transactions),\n",
        "    'transaction_type': np.random.choice(['Purchase', 'Withdrawal', 'Transfer'], n_transactions, p=[0.7, 0.2, 0.1]),\n",
        "    'location_risk': np.random.choice(['Low', 'Medium', 'High'], n_transactions, p=[0.8, 0.15, 0.05]),\n",
        "    'is_weekend': np.random.choice([0, 1], n_transactions, p=[0.7, 0.3]),\n",
        "    'card_present': np.random.choice([0, 1], n_transactions, p=[0.4, 0.6]),\n",
        "}\n",
        "\n",
        "finance_df = pd.DataFrame(transaction_data)\n",
        "print(f\"✅ Created {n_transactions} financial transactions\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7.1 Financial Feature Engineering\n",
        "# ----------------------------------------\n",
        "print(\"\\n7.1 Creating financial risk and behavior features...\")\n",
        "\n",
        "def create_financial_features(df):\n",
        "    \"\"\"\n",
        "    Create financial risk and behavioral features\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Time-based features\n",
        "    df['hour'] = df['timestamp'].dt.hour\n",
        "    df['day_of_week'] = df['timestamp'].dt.dayofweek\n",
        "    df['month'] = df['timestamp'].dt.month\n",
        "    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] <= 17)).astype(int)\n",
        "    df['is_late_night'] = ((df['hour'] >= 22) | (df['hour'] <= 6)).astype(int)\n",
        "\n",
        "    # Amount-based features\n",
        "    df['amount_log'] = np.log1p(df['amount'])  # Log transformation for skewed amounts\n",
        "    df['is_round_amount'] = (df['amount'] % 10 == 0).astype(int)  # Round amounts can indicate fraud\n",
        "\n",
        "    # Amount categories\n",
        "    def categorize_amount(amount):\n",
        "        if amount < 10:\n",
        "            return 'Micro'\n",
        "        elif amount < 100:\n",
        "            return 'Small'\n",
        "        elif amount < 1000:\n",
        "            return 'Medium'\n",
        "        else:\n",
        "            return 'Large'\n",
        "\n",
        "    df['amount_category'] = df['amount'].apply(categorize_amount)\n",
        "\n",
        "    return df\n",
        "\n",
        "finance_with_features = create_financial_features(finance_df)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7.2 Customer Behavior Analysis\n",
        "# ----------------------------------------\n",
        "print(\"\\n7.2 Creating customer behavior patterns...\")\n",
        "\n",
        "def create_customer_features(df):\n",
        "    \"\"\"\n",
        "    Create customer-level behavioral features\n",
        "    \"\"\"\n",
        "    # Calculate customer-level statistics\n",
        "    customer_stats = df.groupby('customer_id').agg({\n",
        "        'amount': ['count', 'sum', 'mean', 'std'],\n",
        "        'timestamp': ['min', 'max'],\n",
        "        'merchant_category': lambda x: x.nunique(),  # Number of different merchant types\n",
        "        'location_risk': lambda x: (x == 'High').sum(),  # High-risk location count\n",
        "        'is_weekend': 'sum',\n",
        "        'card_present': 'mean'\n",
        "    }).round(3)\n",
        "\n",
        "    # Flatten column names\n",
        "    customer_stats.columns = ['_'.join(col).strip() for col in customer_stats.columns]\n",
        "    customer_stats = customer_stats.reset_index()\n",
        "\n",
        "    # Calculate additional customer features with safe division\n",
        "    customer_stats['avg_days_between_transactions'] = (\n",
        "        (customer_stats['timestamp_max'] - customer_stats['timestamp_min']).dt.days /\n",
        "        np.maximum(customer_stats['amount_count'], 1)  # Prevent division by zero\n",
        "    ).fillna(1)  # Fill NaN with 1 day\n",
        "\n",
        "    # Prevent division by zero in spending velocity calculation\n",
        "    customer_stats['spending_velocity'] = customer_stats['amount_sum'] / np.maximum(\n",
        "        customer_stats['avg_days_between_transactions'], 0.1\n",
        "    )\n",
        "\n",
        "    customer_stats['merchant_diversity'] = customer_stats['merchant_category_<lambda>']\n",
        "    customer_stats['risk_transaction_ratio'] = customer_stats['location_risk_<lambda>'] / np.maximum(\n",
        "        customer_stats['amount_count'], 1\n",
        "    )\n",
        "\n",
        "    # Handle any remaining infinity or extreme values\n",
        "    for col in ['spending_velocity', 'risk_transaction_ratio']:\n",
        "        if col in customer_stats.columns:\n",
        "            # Replace infinity with a reasonable maximum value\n",
        "            max_reasonable = customer_stats[customer_stats[col] != np.inf][col].quantile(0.95) * 2\n",
        "            customer_stats[col] = customer_stats[col].replace([np.inf, -np.inf], max_reasonable)\n",
        "            customer_stats[col] = customer_stats[col].fillna(0)\n",
        "\n",
        "    # Merge back with original data\n",
        "    df_with_customer = df.merge(customer_stats, on='customer_id', how='left')\n",
        "\n",
        "    return df_with_customer\n",
        "\n",
        "finance_with_customer = create_customer_features(finance_with_features)\n",
        "print(\"✅ Customer behavior features created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7.3 Fraud Detection Features\n",
        "# ----------------------------------------\n",
        "print(\"\\n7.3 Creating fraud detection features...\")\n",
        "\n",
        "def create_fraud_features(df):\n",
        "    \"\"\"\n",
        "    Create features commonly used for fraud detection\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['customer_id', 'timestamp']).reset_index(drop=True)\n",
        "\n",
        "    # Velocity features (how fast transactions are happening)\n",
        "    df['time_since_last_transaction'] = df.groupby('customer_id')['timestamp'].diff().dt.total_seconds() / 3600  # Hours\n",
        "    df['velocity_flag'] = (df['time_since_last_transaction'] < 0.5).astype(int)  # Less than 30 minutes\n",
        "\n",
        "    # Amount deviation features\n",
        "    df['amount_zscore_customer'] = df.groupby('customer_id')['amount'].transform(\n",
        "        lambda x: (x - x.mean()) / (x.std() + 1e-6)\n",
        "    )\n",
        "    df['large_amount_flag'] = (df['amount_zscore_customer'] > 2).astype(int)\n",
        "\n",
        "    # Location and time anomalies\n",
        "    df['unusual_time_flag'] = df['is_late_night']\n",
        "    df['high_risk_location_flag'] = (df['location_risk'] == 'High').astype(int)\n",
        "\n",
        "    # Merchant category deviation\n",
        "    customer_usual_categories = df.groupby('customer_id')['merchant_category'].apply(\n",
        "        lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Unknown'\n",
        "    )\n",
        "    df['usual_merchant_category'] = df['customer_id'].map(customer_usual_categories)\n",
        "    df['unusual_merchant_flag'] = (df['merchant_category'] != df['usual_merchant_category']).astype(int)\n",
        "\n",
        "    # Composite risk score\n",
        "    df['fraud_risk_score'] = (\n",
        "        df['velocity_flag'] * 0.3 +\n",
        "        df['large_amount_flag'] * 0.3 +\n",
        "        df['unusual_time_flag'] * 0.2 +\n",
        "        df['high_risk_location_flag'] * 0.15 +\n",
        "        df['unusual_merchant_flag'] * 0.05\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "finance_final = create_fraud_features(finance_with_customer)\n",
        "print(\"✅ Fraud detection features created\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 7.4 Encoding and Standardization\n",
        "# ----------------------------------------\n",
        "print(\"\\n7.4 Encoding categorical variables and standardizing...\")\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "categorical_cols = ['merchant_category', 'transaction_type', 'location_risk', 'amount_category']\n",
        "finance_encoded = pd.get_dummies(finance_final, columns=categorical_cols, prefix=categorical_cols)\n",
        "\n",
        "# Clean and prepare numerical features for standardization\n",
        "print(\"🧹 Cleaning numerical data before standardization...\")\n",
        "\n",
        "def clean_numerical_data(df, columns):\n",
        "    \"\"\"\n",
        "    Clean numerical data to handle infinity, NaN, and extreme values\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        if col in df_clean.columns:\n",
        "            # Replace infinity values with NaN\n",
        "            df_clean[col] = df_clean[col].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "            # Fill NaN values with appropriate defaults\n",
        "            if col in ['amount_count', 'amount_sum', 'amount_mean']:\n",
        "                # For customer aggregations, use 0 for missing values\n",
        "                df_clean[col] = df_clean[col].fillna(0)\n",
        "            elif col == 'spending_velocity':\n",
        "                # For velocity, use median of non-zero values\n",
        "                non_zero_median = df_clean[df_clean[col] > 0][col].median()\n",
        "                df_clean[col] = df_clean[col].fillna(non_zero_median if not pd.isna(non_zero_median) else 0)\n",
        "            elif col in ['time_since_last_transaction']:\n",
        "                # For time differences, use median\n",
        "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "            else:\n",
        "                # For other columns, use median\n",
        "                df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
        "\n",
        "            # Cap extreme values at 99th percentile to handle outliers\n",
        "            if df_clean[col].std() > 0:  # Only if there's variation\n",
        "                upper_limit = df_clean[col].quantile(0.99)\n",
        "                lower_limit = df_clean[col].quantile(0.01)\n",
        "                df_clean[col] = df_clean[col].clip(lower=lower_limit, upper=upper_limit)\n",
        "\n",
        "    return df_clean\n",
        "\n",
        "# Identify available numerical features (some might not exist due to data generation)\n",
        "available_numerical_features = []\n",
        "potential_features = ['amount', 'amount_log', 'hour', 'amount_count', 'amount_sum',\n",
        "                     'amount_mean', 'spending_velocity', 'fraud_risk_score', 'time_since_last_transaction']\n",
        "\n",
        "for feature in potential_features:\n",
        "    if feature in finance_encoded.columns:\n",
        "        available_numerical_features.append(feature)\n",
        "\n",
        "print(f\"Available numerical features: {available_numerical_features}\")\n",
        "\n",
        "# Clean the data\n",
        "finance_cleaned = clean_numerical_data(finance_encoded, available_numerical_features)\n",
        "\n",
        "# Check for any remaining issues\n",
        "print(\"🔍 Checking for data quality issues...\")\n",
        "for col in available_numerical_features:\n",
        "    if col in finance_cleaned.columns:\n",
        "        has_inf = np.isinf(finance_cleaned[col]).any()\n",
        "        has_nan = finance_cleaned[col].isna().any()\n",
        "        print(f\"  {col}: inf={has_inf}, nan={has_nan}, range=({finance_cleaned[col].min():.2f}, {finance_cleaned[col].max():.2f})\")\n",
        "\n",
        "# Standardize numerical features\n",
        "scaler_finance = StandardScaler()\n",
        "finance_scaled = finance_cleaned.copy()\n",
        "\n",
        "if available_numerical_features:\n",
        "    try:\n",
        "        finance_scaled[available_numerical_features] = scaler_finance.fit_transform(\n",
        "            finance_cleaned[available_numerical_features]\n",
        "        )\n",
        "        print(\"✅ Financial data encoded and standardized successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️  Standardization warning: {e}\")\n",
        "        print(\"Proceeding with non-standardized numerical features...\")\n",
        "        finance_scaled = finance_cleaned.copy()\n",
        "else:\n",
        "    print(\"⚠️  No numerical features found for standardization\")\n",
        "    finance_scaled = finance_cleaned.copy()\n",
        "\n",
        "print(f\"Final feature count: {len(finance_scaled.columns)}\")\n",
        "\n",
        "# Display finance data conversion summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINANCE DATA CONVERSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(\"Conversions performed:\")\n",
        "print(\"- Time-based feature extraction\")\n",
        "print(\"- Amount categorization and log transformation\")\n",
        "print(\"- Customer behavior profiling\")\n",
        "print(\"- Fraud risk scoring\")\n",
        "print(\"- Velocity and anomaly detection\")\n",
        "print(\"- Categorical encoding and standardization\")\n",
        "\n",
        "sample_finance_features = ['transaction_id', 'amount', 'merchant_category', 'fraud_risk_score',\n",
        "                          'velocity_flag', 'large_amount_flag']\n",
        "print(\"\\nSample converted financial data:\")\n",
        "print(finance_final[sample_finance_features].head())\n",
        "\n",
        "# ========================================\n",
        "# 8. BEST PRACTICES IMPLEMENTATION\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"8. DATA CONVERSION BEST PRACTICES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8.1 Data Quality Assessment\n",
        "# ----------------------------------------\n",
        "print(\"\\n8.1 Implementing data quality assessment...\")\n",
        "\n",
        "def assess_data_quality(df, dataset_name):\n",
        "    \"\"\"\n",
        "    Comprehensive data quality assessment\n",
        "    \"\"\"\n",
        "    print(f\"\\n📊 Data Quality Report for {dataset_name}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Basic statistics\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Total cells: {df.shape[0] * df.shape[1]:,}\")\n",
        "\n",
        "    # Missing values analysis\n",
        "    missing_counts = df.isnull().sum()\n",
        "    missing_percentage = (missing_counts / len(df)) * 100\n",
        "\n",
        "    if missing_counts.sum() > 0:\n",
        "        print(f\"\\n⚠️  Missing Values Found:\")\n",
        "        for col in missing_counts[missing_counts > 0].index:\n",
        "            print(f\"  {col}: {missing_counts[col]} ({missing_percentage[col]:.1f}%)\")\n",
        "    else:\n",
        "        print(\"✅ No missing values found\")\n",
        "\n",
        "    # Data type analysis\n",
        "    print(f\"\\nData Types:\")\n",
        "    for dtype in df.dtypes.value_counts().index:\n",
        "        count = df.dtypes.value_counts()[dtype]\n",
        "        print(f\"  {dtype}: {count} columns\")\n",
        "\n",
        "    # Potential outliers (for numerical columns)\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numerical_cols) > 0:\n",
        "        print(f\"\\n📈 Numerical Columns Analysis:\")\n",
        "        for col in numerical_cols[:5]:  # Show first 5 numerical columns\n",
        "            q1 = df[col].quantile(0.25)\n",
        "            q3 = df[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            outliers = ((df[col] < (q1 - 1.5 * iqr)) | (df[col] > (q3 + 1.5 * iqr))).sum()\n",
        "            print(f\"  {col}: {outliers} potential outliers ({outliers/len(df)*100:.1f}%)\")\n",
        "\n",
        "    # Memory usage\n",
        "    memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "    print(f\"\\n💾 Memory Usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "    return {\n",
        "        'missing_values': missing_counts.sum(),\n",
        "        'missing_percentage': missing_percentage.max(),\n",
        "        'potential_outliers': outliers if 'outliers' in locals() else 0,\n",
        "        'memory_mb': memory_mb\n",
        "    }\n",
        "\n",
        "# Assess quality of our example datasets\n",
        "quality_reports = {}\n",
        "quality_reports['customer'] = assess_data_quality(df, \"Customer Data\")\n",
        "quality_reports['reviews'] = assess_data_quality(reviews_with_features, \"Review Data\")\n",
        "quality_reports['healthcare'] = assess_data_quality(patients_with_features, \"Healthcare Data\")\n",
        "quality_reports['finance'] = assess_data_quality(finance_final, \"Finance Data\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8.2 Validation Framework\n",
        "# ----------------------------------------\n",
        "print(\"\\n8.2 Implementing data validation framework...\")\n",
        "\n",
        "def validate_converted_data(df, validation_rules):\n",
        "    \"\"\"\n",
        "    Validate converted data against business rules\n",
        "    \"\"\"\n",
        "    validation_results = {}\n",
        "\n",
        "    for rule_name, rule_function in validation_rules.items():\n",
        "        try:\n",
        "            result = rule_function(df)\n",
        "            validation_results[rule_name] = {'passed': result, 'error': None}\n",
        "        except Exception as e:\n",
        "            validation_results[rule_name] = {'passed': False, 'error': str(e)}\n",
        "\n",
        "    return validation_results\n",
        "\n",
        "# Define validation rules for healthcare data\n",
        "healthcare_validation_rules = {\n",
        "    'bmi_reasonable': lambda df: df['bmi'].between(10, 60).all(),\n",
        "    'age_valid': lambda df: df['age'].between(0, 120).all(),\n",
        "    'bp_logical': lambda df: (df['systolic_bp'] > df['diastolic_bp']).all(),\n",
        "    'risk_score_range': lambda df: df['diabetes_risk_score'].between(0, 7).all(),\n",
        "}\n",
        "\n",
        "# Define validation rules for finance data\n",
        "finance_validation_rules = {\n",
        "    'positive_amounts': lambda df: (df['amount'] > 0).all(),\n",
        "    'valid_timestamps': lambda df: df['timestamp'].notna().all(),\n",
        "    'fraud_score_range': lambda df: df['fraud_risk_score'].between(0, 1).all(),\n",
        "}\n",
        "\n",
        "# Run validations\n",
        "print(\"🔍 Running healthcare data validation...\")\n",
        "healthcare_validation = validate_converted_data(patients_with_features, healthcare_validation_rules)\n",
        "for rule, result in healthcare_validation.items():\n",
        "    status = \"✅ PASS\" if result['passed'] else f\"❌ FAIL: {result['error']}\"\n",
        "    print(f\"  {rule}: {status}\")\n",
        "\n",
        "print(\"\\n🔍 Running finance data validation...\")\n",
        "finance_validation = validate_converted_data(finance_final, finance_validation_rules)\n",
        "for rule, result in finance_validation.items():\n",
        "    status = \"✅ PASS\" if result['passed'] else f\"❌ FAIL: {result['error']}\"\n",
        "    print(f\"  {rule}: {status}\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8.3 Feature Engineering Pipeline\n",
        "# ----------------------------------------\n",
        "print(\"\\n8.3 Implementing reusable feature engineering pipeline...\")\n",
        "\n",
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"\n",
        "    Reusable pipeline for feature engineering\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scalers = {}\n",
        "        self.encoders = {}\n",
        "        self.feature_names = []\n",
        "\n",
        "    def fit_transform(self, df, categorical_cols=None, numerical_cols=None):\n",
        "        \"\"\"\n",
        "        Fit and transform data\n",
        "        \"\"\"\n",
        "        df_processed = df.copy()\n",
        "\n",
        "        # Handle categorical variables\n",
        "        if categorical_cols:\n",
        "            for col in categorical_cols:\n",
        "                if col in df_processed.columns:\n",
        "                    # One-hot encoding\n",
        "                    encoded = pd.get_dummies(df_processed[col], prefix=col)\n",
        "                    df_processed = pd.concat([df_processed, encoded], axis=1)\n",
        "                    df_processed.drop(col, axis=1, inplace=True)\n",
        "\n",
        "        # Handle numerical variables\n",
        "        if numerical_cols:\n",
        "            scaler = StandardScaler()\n",
        "            df_processed[numerical_cols] = scaler.fit_transform(df_processed[numerical_cols])\n",
        "            self.scalers['standard'] = scaler\n",
        "\n",
        "        self.feature_names = list(df_processed.columns)\n",
        "        return df_processed\n",
        "\n",
        "    def transform(self, df):\n",
        "        \"\"\"\n",
        "        Transform new data using fitted parameters\n",
        "        \"\"\"\n",
        "        # This would implement the same transformations using stored parameters\n",
        "        # Implementation depends on specific use case\n",
        "        pass\n",
        "\n",
        "    def get_feature_importance_analysis(self, target_col=None):\n",
        "        \"\"\"\n",
        "        Analyze feature importance (placeholder for actual implementation)\n",
        "        \"\"\"\n",
        "        return f\"Feature importance analysis would be implemented here for {len(self.feature_names)} features\"\n",
        "\n",
        "# Demonstrate pipeline usage\n",
        "pipeline = FeatureEngineeringPipeline()\n",
        "print(\"🔧 Feature engineering pipeline created\")\n",
        "print(\"✅ Pipeline ready for production use\")\n",
        "\n",
        "# ----------------------------------------\n",
        "# 8.4 Documentation Generation\n",
        "# ----------------------------------------\n",
        "print(\"\\n8.4 Generating feature documentation...\")\n",
        "\n",
        "def generate_feature_documentation(df, dataset_name):\n",
        "    \"\"\"\n",
        "    Generate comprehensive feature documentation\n",
        "    \"\"\"\n",
        "    doc = f\"\\n📋 FEATURE DOCUMENTATION: {dataset_name.upper()}\\n\"\n",
        "    doc += \"=\" * 60 + \"\\n\"\n",
        "\n",
        "    # Basic info\n",
        "    doc += f\"Dataset: {dataset_name}\\n\"\n",
        "    doc += f\"Features: {len(df.columns)}\\n\"\n",
        "    doc += f\"Records: {len(df):,}\\n\"\n",
        "    doc += f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\"\n",
        "\n",
        "    # Feature types\n",
        "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "    boolean_features = df.select_dtypes(include=['bool']).columns.tolist()\n",
        "\n",
        "    doc += f\"Numerical Features ({len(numerical_features)}):\\n\"\n",
        "    for feature in numerical_features[:10]:  # Show first 10\n",
        "        doc += f\"  - {feature}\\n\"\n",
        "    if len(numerical_features) > 10:\n",
        "        doc += f\"  ... and {len(numerical_features) - 10} more\\n\"\n",
        "\n",
        "    doc += f\"\\nCategorical Features ({len(categorical_features)}):\\n\"\n",
        "    for feature in categorical_features[:10]:\n",
        "        doc += f\"  - {feature}\\n\"\n",
        "    if len(categorical_features) > 10:\n",
        "        doc += f\"  ... and {len(categorical_features) - 10} more\\n\"\n",
        "\n",
        "    doc += f\"\\nBoolean Features ({len(boolean_features)}):\\n\"\n",
        "    for feature in boolean_features:\n",
        "        doc += f\"  - {feature}\\n\"\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Generate documentation for our datasets\n",
        "print(generate_feature_documentation(patients_with_features, \"Healthcare Patient Data\"))\n",
        "print(generate_feature_documentation(finance_final, \"Financial Transaction Data\"))\n",
        "\n",
        "# ========================================\n",
        "# FINAL SUMMARY AND NEXT STEPS\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎉 DATA CONVERSION TUTORIAL COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\"\"\n",
        "WHAT YOU'VE LEARNED:\n",
        "├── Basic data type conversions (text, numbers, dates)\n",
        "├── Text data processing (sentiment analysis, vectorization)\n",
        "├── Image data conversion (preprocessing, feature extraction)\n",
        "├── Time series feature engineering (lags, rolling windows)\n",
        "├── Healthcare data conversion (medical risk scoring)\n",
        "├── Financial data conversion (fraud detection features)\n",
        "└── Best practices (validation, pipelines, documentation)\n",
        "\n",
        "TOTAL FEATURES CREATED:\n",
        "\"\"\")\n",
        "\n",
        "feature_counts = {\n",
        "    'Customer Data': len(df.columns),\n",
        "    'Review Data': len(reviews_with_features.columns) + len(tfidf_df.columns),\n",
        "    'Healthcare Data': len(patients_encoded.columns),\n",
        "    'Financial Data': len(finance_scaled.columns),\n",
        "    'Time Series Data': len(stock_data_final.columns)\n",
        "}\n",
        "\n",
        "for dataset, count in feature_counts.items():\n",
        "    print(f\"├── {dataset}: {count} features\")\n",
        "\n",
        "total_features = sum(feature_counts.values())\n",
        "print(f\"└── TOTAL: {total_features} features across all examples!\")\n",
        "\n",
        "print(f\"\"\"\n",
        "NEXT STEPS FOR YOUR AI PROJECTS:\n",
        "1. 🎯 Choose your specific domain (healthcare, finance, etc.)\n",
        "2. 📊 Gather and assess your real data quality\n",
        "3. 🔧 Implement appropriate conversion techniques from this tutorial\n",
        "4. ✅ Set up validation and monitoring systems\n",
        "5. 🚀 Deploy your converted data to AI models\n",
        "6. 📈 Monitor and iterate on your conversion pipeline\n",
        "\n",
        "REMEMBER:\n",
        "- Data conversion is often 80% of the AI project effort\n",
        "- Good conversion leads to better AI model performance\n",
        "- Always validate your converted data before using it\n",
        "- Document everything for future team members\n",
        "- Start simple and add complexity gradually\n",
        "\n",
        "Happy AI building! 🤖✨\n",
        "\"\"\")\n",
        "\n",
        "print(\"💡 TIP: Save this notebook and use it as a reference for your own projects!\")\n",
        "print(\"📚 For more advanced techniques, refer to the companion deep dive document.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xIUTVKsP--uk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}