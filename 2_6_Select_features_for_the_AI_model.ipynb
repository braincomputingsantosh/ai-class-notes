{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tL_PZ2tQpOYb"
      },
      "outputs": [],
      "source": [
        "# Feature Selection in AI - Practical Examples\n",
        "# Google Colab Notebook\n",
        "# This notebook demonstrates feature selection techniques with real-world examples\n",
        "\n",
        "# First, let's install and import the necessary libraries\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn plotly\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE, SelectFromModel\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üìö This notebook will guide you through feature selection step-by-step\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 1: Understanding Features with a Simple Example\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 1: Understanding What Features Are\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Let's create a simple dataset to understand features\n",
        "# Imagine we're predicting whether someone will buy a product\n",
        "\n",
        "# Create sample data\n",
        "np.random.seed(42)\n",
        "n_customers = 1000\n",
        "\n",
        "# Generate customer features\n",
        "age = np.random.normal(35, 12, n_customers)\n",
        "income = np.random.normal(50000, 15000, n_customers)\n",
        "website_visits = np.random.poisson(5, n_customers)\n",
        "previous_purchases = np.random.poisson(2, n_customers)\n",
        "random_noise = np.random.normal(0, 1, n_customers)  # This should be irrelevant\n",
        "\n",
        "# Create purchase decision (this is what we want to predict)\n",
        "# Let's say purchase depends on age, income, and previous purchases\n",
        "purchase_probability = (\n",
        "    0.3 * (age - 20) / 20 +  # Older people more likely to buy\n",
        "    0.4 * (income - 30000) / 30000 +  # Higher income more likely to buy\n",
        "    0.3 * previous_purchases / 5 +  # Previous customers more likely to buy\n",
        "    0.1 * np.random.normal(0, 1, n_customers)  # Add some randomness\n",
        ")\n",
        "\n",
        "# Convert to binary purchase decision\n",
        "will_purchase = (purchase_probability > 0.5).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "customer_data = pd.DataFrame({\n",
        "    'age': age,\n",
        "    'income': income,\n",
        "    'website_visits': website_visits,\n",
        "    'previous_purchases': previous_purchases,\n",
        "    'random_noise': random_noise,\n",
        "    'will_purchase': will_purchase\n",
        "})\n",
        "\n",
        "print(\"üìä Sample Customer Data:\")\n",
        "print(customer_data.head())\n",
        "print(f\"\\nüìà Dataset shape: {customer_data.shape}\")\n",
        "print(f\"üìà Purchase rate: {customer_data['will_purchase'].mean():.2%}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 2: Visualizing Feature Relationships\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 2: Visualizing How Features Relate to Our Target\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create visualizations to understand feature relationships\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "fig.suptitle('Feature Relationships with Purchase Decision', fontsize=16)\n",
        "\n",
        "features = ['age', 'income', 'website_visits', 'previous_purchases', 'random_noise']\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "\n",
        "    # Create box plots to show distribution by purchase decision\n",
        "    customer_data.boxplot(column=feature, by='will_purchase', ax=axes[row, col])\n",
        "    axes[row, col].set_title(f'{feature.title()} vs Purchase Decision')\n",
        "    axes[row, col].set_xlabel('Will Purchase (0=No, 1=Yes)')\n",
        "\n",
        "# Remove the empty subplot\n",
        "fig.delaxes(axes[1, 2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlations\n",
        "correlations = customer_data.corr()['will_purchase'].sort_values(ascending=False)\n",
        "print(\"\\nüìä Feature Correlations with Purchase Decision:\")\n",
        "for feature, corr in correlations.items():\n",
        "    if feature != 'will_purchase':\n",
        "        print(f\"{feature:20s}: {corr:6.3f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 3: Manual Feature Selection Based on Domain Knowledge\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 3: Manual Feature Selection (Domain Expert Approach)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Let's pretend we're domain experts and manually select features\n",
        "# Based on business logic, we know that:\n",
        "# 1. Age matters (older people might have more disposable income)\n",
        "# 2. Income matters (higher income = more purchasing power)\n",
        "# 3. Previous purchases matter (loyalty/satisfaction)\n",
        "# 4. Website visits might matter (engagement)\n",
        "# 5. Random noise should NOT matter\n",
        "\n",
        "manually_selected_features = ['age', 'income', 'previous_purchases', 'website_visits']\n",
        "print(\"üß† Domain Expert Selection:\")\n",
        "print(\"‚úÖ Selected features:\", manually_selected_features)\n",
        "print(\"‚ùå Excluded features: random_noise (obviously irrelevant)\")\n",
        "\n",
        "# Split data for manual selection\n",
        "X_manual = customer_data[manually_selected_features]\n",
        "y = customer_data['will_purchase']\n",
        "\n",
        "X_train_manual, X_test_manual, y_train, y_test = train_test_split(\n",
        "    X_manual, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train a simple model with manually selected features\n",
        "manual_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "manual_model.fit(X_train_manual, y_train)\n",
        "\n",
        "# Evaluate performance\n",
        "manual_predictions = manual_model.predict(X_test_manual)\n",
        "manual_accuracy = accuracy_score(y_test, manual_predictions)\n",
        "\n",
        "print(f\"\\nüìä Manual Selection Results:\")\n",
        "print(f\"Accuracy: {manual_accuracy:.3f}\")\n",
        "print(f\"Features used: {len(manually_selected_features)}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 4: Statistical Feature Selection Methods\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 4: Statistical Feature Selection Methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Prepare all features for statistical selection\n",
        "all_features = ['age', 'income', 'website_visits', 'previous_purchases', 'random_noise']\n",
        "X_all = customer_data[all_features]\n",
        "\n",
        "# Method 1: SelectKBest (Univariate Selection)\n",
        "print(\"üîç Method 1: SelectKBest (Univariate Statistical Tests)\")\n",
        "print(\"This method selects features based on statistical tests\")\n",
        "\n",
        "selector_kbest = SelectKBest(score_func=f_classif, k=3)\n",
        "X_selected_kbest = selector_kbest.fit_transform(X_all, y)\n",
        "\n",
        "# Get selected feature names\n",
        "selected_features_kbest = [all_features[i] for i in selector_kbest.get_support(indices=True)]\n",
        "feature_scores = selector_kbest.scores_\n",
        "\n",
        "print(\"\\nüìä Feature Scores (higher is better):\")\n",
        "for feature, score in zip(all_features, feature_scores):\n",
        "    selected = \"‚úÖ\" if feature in selected_features_kbest else \"‚ùå\"\n",
        "    print(f\"{selected} {feature:20s}: {score:8.2f}\")\n",
        "\n",
        "# Method 2: Recursive Feature Elimination (RFE)\n",
        "print(\"\\nüîç Method 2: Recursive Feature Elimination (RFE)\")\n",
        "print(\"This method recursively removes features and builds models\")\n",
        "\n",
        "estimator = LogisticRegression(random_state=42)\n",
        "selector_rfe = RFE(estimator, n_features_to_select=3)\n",
        "X_selected_rfe = selector_rfe.fit_transform(X_all, y)\n",
        "\n",
        "selected_features_rfe = [all_features[i] for i in selector_rfe.get_support(indices=True)]\n",
        "feature_rankings = selector_rfe.ranking_\n",
        "\n",
        "print(\"\\nüìä Feature Rankings (1 is best):\")\n",
        "for feature, rank in zip(all_features, feature_rankings):\n",
        "    selected = \"‚úÖ\" if feature in selected_features_rfe else \"‚ùå\"\n",
        "    print(f\"{selected} {feature:20s}: Rank {rank}\")\n",
        "\n",
        "# Method 3: Model-Based Selection (Feature Importance)\n",
        "print(\"\\nüîç Method 3: Model-Based Selection (Feature Importance)\")\n",
        "print(\"This method uses a model to determine feature importance\")\n",
        "\n",
        "selector_model = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
        "X_selected_model = selector_model.fit_transform(X_all, y)\n",
        "\n",
        "selected_features_model = [all_features[i] for i in selector_model.get_support(indices=True)]\n",
        "feature_importances = selector_model.estimator_.feature_importances_\n",
        "\n",
        "print(\"\\nüìä Feature Importance (higher is better):\")\n",
        "for feature, importance in zip(all_features, feature_importances):\n",
        "    selected = \"‚úÖ\" if feature in selected_features_model else \"‚ùå\"\n",
        "    print(f\"{selected} {feature:20s}: {importance:.4f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 5: Comparing Feature Selection Methods\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 5: Comparing Different Feature Selection Methods\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Compare all methods\n",
        "methods = {\n",
        "    'Manual Selection': manually_selected_features,\n",
        "    'SelectKBest': selected_features_kbest,\n",
        "    'RFE': selected_features_rfe,\n",
        "    'Model-Based': selected_features_model\n",
        "}\n",
        "\n",
        "print(\"üîç Comparison of Feature Selection Methods:\")\n",
        "for method_name, selected_features in methods.items():\n",
        "    print(f\"\\n{method_name}:\")\n",
        "    print(f\"  Selected: {selected_features}\")\n",
        "    print(f\"  Count: {len(selected_features)}\")\n",
        "\n",
        "# Test each method's performance\n",
        "results = {}\n",
        "\n",
        "for method_name, selected_features in methods.items():\n",
        "    # Prepare data\n",
        "    X_method = customer_data[selected_features]\n",
        "    X_train_method, X_test_method, _, _ = train_test_split(\n",
        "        X_method, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "    model.fit(X_train_method, y_train)\n",
        "\n",
        "    # Evaluate\n",
        "    predictions = model.predict(X_test_method)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    results[method_name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'num_features': len(selected_features),\n",
        "        'features': selected_features\n",
        "    }\n",
        "\n",
        "print(\"\\nüìä Performance Comparison:\")\n",
        "print(f\"{'Method':<20} {'Accuracy':<10} {'# Features':<12} {'Features'}\")\n",
        "print(\"-\" * 70)\n",
        "for method_name, result in results.items():\n",
        "    print(f\"{method_name:<20} {result['accuracy']:.3f}     {result['num_features']:<12} {result['features']}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 6: Real-World Healthcare Example\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 6: Real-World Example - Healthcare (Breast Cancer Detection)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load breast cancer dataset (classic medical AI example)\n",
        "cancer_data = load_breast_cancer()\n",
        "X_cancer = pd.DataFrame(cancer_data.data, columns=cancer_data.feature_names)\n",
        "y_cancer = cancer_data.target\n",
        "\n",
        "print(f\"üìä Breast Cancer Dataset:\")\n",
        "print(f\"Samples: {X_cancer.shape[0]}\")\n",
        "print(f\"Features: {X_cancer.shape[1]}\")\n",
        "print(f\"Target: {cancer_data.target_names}\")\n",
        "\n",
        "# Show first few feature names\n",
        "print(f\"\\nüîç First 10 features:\")\n",
        "for i, feature in enumerate(X_cancer.columns[:10]):\n",
        "    print(f\"  {i+1:2d}. {feature}\")\n",
        "\n",
        "# This dataset has 30 features - too many to interpret easily\n",
        "# Let's use feature selection to find the most important ones\n",
        "\n",
        "# Apply feature selection\n",
        "print(\"\\nüè• Applying Feature Selection for Medical Diagnosis:\")\n",
        "\n",
        "# Use SelectKBest to find top 10 features\n",
        "selector_medical = SelectKBest(score_func=f_classif, k=10)\n",
        "X_cancer_selected = selector_medical.fit_transform(X_cancer, y_cancer)\n",
        "\n",
        "# Get selected feature names and scores\n",
        "selected_indices = selector_medical.get_support(indices=True)\n",
        "selected_features_medical = [X_cancer.columns[i] for i in selected_indices]\n",
        "medical_scores = selector_medical.scores_\n",
        "\n",
        "print(f\"\\nüìä Top 10 Most Important Features for Cancer Detection:\")\n",
        "feature_score_pairs = [(X_cancer.columns[i], medical_scores[i]) for i in selected_indices]\n",
        "feature_score_pairs.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for i, (feature, score) in enumerate(feature_score_pairs):\n",
        "    print(f\"  {i+1:2d}. {feature:<25} Score: {score:8.2f}\")\n",
        "\n",
        "# Compare model performance with all features vs selected features\n",
        "X_train_all, X_test_all, y_train_cancer, y_test_cancer = train_test_split(\n",
        "    X_cancer, y_cancer, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "X_train_selected, X_test_selected, _, _ = train_test_split(\n",
        "    X_cancer_selected, y_cancer, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train models\n",
        "model_all_features = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model_selected_features = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "model_all_features.fit(X_train_all, y_train_cancer)\n",
        "model_selected_features.fit(X_train_selected, y_train_cancer)\n",
        "\n",
        "# Evaluate both models\n",
        "pred_all = model_all_features.predict(X_test_all)\n",
        "pred_selected = model_selected_features.predict(X_test_selected)\n",
        "\n",
        "acc_all = accuracy_score(y_test_cancer, pred_all)\n",
        "acc_selected = accuracy_score(y_test_cancer, pred_selected)\n",
        "\n",
        "print(f\"\\nüìä Medical Diagnosis Results:\")\n",
        "print(f\"All features ({X_cancer.shape[1]} features):      Accuracy = {acc_all:.3f}\")\n",
        "print(f\"Selected features (10 features):    Accuracy = {acc_selected:.3f}\")\n",
        "print(f\"Feature reduction: {X_cancer.shape[1]} ‚Üí 10 features ({X_cancer.shape[1]-10} fewer)\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 7: Real-World Finance Example\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 7: Real-World Example - Finance (Credit Risk Assessment)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a realistic credit risk dataset\n",
        "print(\"üí≥ Creating Credit Risk Assessment Dataset...\")\n",
        "\n",
        "np.random.seed(42)\n",
        "n_applicants = 2000\n",
        "\n",
        "# Generate realistic financial features\n",
        "credit_score = np.random.normal(650, 100, n_applicants)\n",
        "annual_income = np.random.lognormal(np.log(50000), 0.5, n_applicants)\n",
        "debt_to_income = np.random.beta(2, 5, n_applicants)  # Most people have reasonable debt\n",
        "employment_years = np.random.exponential(5, n_applicants)\n",
        "num_credit_cards = np.random.poisson(3, n_applicants)\n",
        "monthly_expenses = annual_income * 0.6 * np.random.normal(1, 0.2, n_applicants)\n",
        "savings_balance = np.random.exponential(10000, n_applicants)\n",
        "\n",
        "# Create some irrelevant features (these shouldn't predict default)\n",
        "favorite_color = np.random.choice(['red', 'blue', 'green', 'yellow'], n_applicants)\n",
        "birth_month = np.random.randint(1, 13, n_applicants)\n",
        "lucky_number = np.random.randint(1, 101, n_applicants)\n",
        "\n",
        "# Create default probability based on financial logic\n",
        "default_probability = (\n",
        "    0.3 * np.maximum(0, (700 - credit_score) / 200) +  # Lower credit score = higher risk\n",
        "    0.2 * np.maximum(0, (debt_to_income - 0.4) / 0.4) +  # High debt ratio = higher risk\n",
        "    0.2 * np.maximum(0, (40000 - annual_income) / 40000) +  # Low income = higher risk\n",
        "    0.1 * np.maximum(0, (2 - employment_years) / 2) +  # Short employment = higher risk\n",
        "    0.2 * np.random.beta(2, 8, n_applicants)  # Random component\n",
        ")\n",
        "\n",
        "# Convert to binary default decision\n",
        "will_default = (default_probability > 0.4).astype(int)\n",
        "\n",
        "# Create DataFrame\n",
        "credit_data = pd.DataFrame({\n",
        "    'credit_score': credit_score,\n",
        "    'annual_income': annual_income,\n",
        "    'debt_to_income_ratio': debt_to_income,\n",
        "    'employment_years': employment_years,\n",
        "    'num_credit_cards': num_credit_cards,\n",
        "    'monthly_expenses': monthly_expenses,\n",
        "    'savings_balance': savings_balance,\n",
        "    'favorite_color_encoded': pd.Categorical(favorite_color).codes,\n",
        "    'birth_month': birth_month,\n",
        "    'lucky_number': lucky_number,\n",
        "    'will_default': will_default\n",
        "})\n",
        "\n",
        "print(f\"üìä Credit Risk Dataset:\")\n",
        "print(f\"Applicants: {credit_data.shape[0]}\")\n",
        "print(f\"Features: {credit_data.shape[1] - 1}\")\n",
        "print(f\"Default rate: {credit_data['will_default'].mean():.2%}\")\n",
        "\n",
        "# Apply feature selection for credit risk\n",
        "finance_features = [col for col in credit_data.columns if col != 'will_default']\n",
        "X_finance = credit_data[finance_features]\n",
        "y_finance = credit_data['will_default']\n",
        "\n",
        "# Use multiple feature selection methods\n",
        "print(\"\\nüí∞ Applying Feature Selection for Credit Risk:\")\n",
        "\n",
        "# Method 1: Correlation-based selection\n",
        "correlations_finance = credit_data.corr()['will_default'].abs().sort_values(ascending=False)\n",
        "top_correlated = correlations_finance.head(6).index.tolist()\n",
        "top_correlated.remove('will_default')  # Remove target variable\n",
        "\n",
        "print(\"üìä Top 5 Features by Correlation with Default:\")\n",
        "for i, feature in enumerate(top_correlated, 1):\n",
        "    corr = correlations_finance[feature]\n",
        "    print(f\"  {i}. {feature:<25} Correlation: {corr:.3f}\")\n",
        "\n",
        "# Method 2: Statistical significance\n",
        "selector_finance = SelectKBest(score_func=f_classif, k=5)\n",
        "X_finance_selected = selector_finance.fit_transform(X_finance, y_finance)\n",
        "\n",
        "selected_indices_finance = selector_finance.get_support(indices=True)\n",
        "selected_features_finance = [finance_features[i] for i in selected_indices_finance]\n",
        "finance_scores = selector_finance.scores_\n",
        "\n",
        "print(f\"\\nüìä Top 5 Features by Statistical Significance:\")\n",
        "feature_score_pairs_finance = [(finance_features[i], finance_scores[i]) for i in selected_indices_finance]\n",
        "feature_score_pairs_finance.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "for i, (feature, score) in enumerate(feature_score_pairs_finance, 1):\n",
        "    print(f\"  {i}. {feature:<25} Score: {score:8.2f}\")\n",
        "\n",
        "# Compare performance\n",
        "X_train_finance, X_test_finance, y_train_finance, y_test_finance = train_test_split(\n",
        "    X_finance[selected_features_finance], y_finance, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "finance_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "finance_model.fit(X_train_finance, y_train_finance)\n",
        "\n",
        "finance_predictions = finance_model.predict(X_test_finance)\n",
        "finance_accuracy = accuracy_score(y_test_finance, finance_predictions)\n",
        "\n",
        "print(f\"\\nüìä Credit Risk Model Results:\")\n",
        "print(f\"Selected features: {selected_features_finance}\")\n",
        "print(f\"Accuracy: {finance_accuracy:.3f}\")\n",
        "\n",
        "# Show feature importance\n",
        "feature_importance_finance = finance_model.feature_importances_\n",
        "print(f\"\\nüìä Feature Importance in Final Model:\")\n",
        "for feature, importance in zip(selected_features_finance, feature_importance_finance):\n",
        "    print(f\"  {feature:<25} Importance: {importance:.3f}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 8: Practical Tips and Best Practices\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 8: Practical Tips and Best Practices\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üí° KEY TAKEAWAYS FROM OUR EXAMPLES:\")\n",
        "print()\n",
        "print(\"1. üéØ DOMAIN KNOWLEDGE IS CRUCIAL\")\n",
        "print(\"   - Our manual selection often performed as well as statistical methods\")\n",
        "print(\"   - Subject matter experts can identify relevant features that statistics might miss\")\n",
        "print(\"   - Always validate statistical results with domain expertise\")\n",
        "print()\n",
        "print(\"2. üìä DIFFERENT METHODS COMPLEMENT EACH OTHER\")\n",
        "print(\"   - Statistical methods (SelectKBest, RFE) find data patterns\")\n",
        "print(\"   - Model-based methods find features that work well together\")\n",
        "print(\"   - Correlation analysis reveals direct relationships\")\n",
        "print()\n",
        "print(\"3. üîç QUALITY OVER QUANTITY\")\n",
        "print(\"   - 10 good features often beat 100 mediocre ones\")\n",
        "print(\"   - Our cancer detection worked well with just 10 out of 30 features\")\n",
        "print(\"   - Fewer features = faster training, easier interpretation\")\n",
        "print()\n",
        "print(\"4. üè• REAL-WORLD CONSTRAINTS MATTER\")\n",
        "print(\"   - Healthcare: Features must be clinically meaningful\")\n",
        "print(\"   - Finance: Features must be legally compliant and ethically sound\")\n",
        "print(\"   - Consider data collection costs and availability\")\n",
        "print()\n",
        "print(\"5. üîÑ ITERATION IS KEY\")\n",
        "print(\"   - Feature selection is not a one-time process\")\n",
        "print(\"   - Test different combinations and methods\")\n",
        "print(\"   - Monitor performance over time and adjust as needed\")\n",
        "\n",
        "print(\"\\nüõ†Ô∏è PRACTICAL IMPLEMENTATION CHECKLIST:\")\n",
        "print()\n",
        "print(\"Before Starting:\")\n",
        "print(\"‚úÖ Understand your problem domain thoroughly\")\n",
        "print(\"‚úÖ Consult with subject matter experts\")\n",
        "print(\"‚úÖ Understand data collection constraints\")\n",
        "print()\n",
        "print(\"During Feature Selection:\")\n",
        "print(\"‚úÖ Start with domain knowledge\")\n",
        "print(\"‚úÖ Apply multiple statistical methods\")\n",
        "print(\"‚úÖ Check for data leakage and unrealistic features\")\n",
        "print(\"‚úÖ Consider feature interactions, not just individual features\")\n",
        "print()\n",
        "print(\"After Feature Selection:\")\n",
        "print(\"‚úÖ Validate results with domain experts\")\n",
        "print(\"‚úÖ Test on completely new data\")\n",
        "print(\"‚úÖ Monitor performance over time\")\n",
        "print(\"‚úÖ Document your decisions and rationale\")\n",
        "\n",
        "# ==============================================================================\n",
        "# SECTION 9: Interactive Exercise\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 9: Try It Yourself!\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"üéØ EXERCISE: Create Your Own Feature Selection Pipeline\")\n",
        "print()\n",
        "print(\"Below is a template you can modify to try feature selection on your own data:\")\n",
        "print()\n",
        "\n",
        "# Template code for users to modify\n",
        "template_code = '''\n",
        "# TEMPLATE: Feature Selection Pipeline\n",
        "# Modify this code to work with your own dataset\n",
        "\n",
        "def feature_selection_pipeline(X, y, target_name=\"target\"):\n",
        "    \"\"\"\n",
        "    Complete feature selection pipeline\n",
        "\n",
        "    Parameters:\n",
        "    X: DataFrame with features\n",
        "    y: Target variable\n",
        "    target_name: Name of what you're predicting\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"üîç Feature Selection for {target_name}\")\n",
        "    print(f\"Dataset shape: {X.shape}\")\n",
        "\n",
        "    # Step 1: Basic statistics\n",
        "    print(\"\\\\nüìä Basic Feature Statistics:\")\n",
        "    print(X.describe())\n",
        "\n",
        "    # Step 2: Correlation analysis\n",
        "    if len(X.columns) < 20:  # Only show if not too many features\n",
        "        correlations = pd.concat([X, y], axis=1).corr()[y.name].sort_values(ascending=False)\n",
        "        print(f\"\\\\nüìà Correlations with {target_name}:\")\n",
        "        for feature, corr in correlations.items():\n",
        "            if feature != y.name:\n",
        "                print(f\"  {feature:<20}: {corr:6.3f}\")\n",
        "\n",
        "    # Step 3: Statistical feature selection\n",
        "    k = min(10, X.shape[1])  # Select top 10 or all if fewer\n",
        "    selector = SelectKBest(score_func=f_classif, k=k)\n",
        "    X_selected = selector.fit_transform(X, y)\n",
        "\n",
        "    selected_features = [X.columns[i] for i in selector.get_support(indices=True)]\n",
        "\n",
        "    print(f\"\\\\n‚úÖ Selected {len(selected_features)} features:\")\n",
        "    for feature in selected_features:\n",
        "        print(f\"  - {feature}\")\n",
        "\n",
        "    return X_selected, selected_features\n",
        "\n",
        "# Example usage with our customer data:\n",
        "X_example = customer_data[['age', 'income', 'website_visits', 'previous_purchases', 'random_noise']]\n",
        "y_example = customer_data['will_purchase']\n",
        "\n",
        "X_selected_example, selected_features_example = feature_selection_pipeline(\n",
        "    X_example, y_example, \"customer purchase\"\n",
        ")\n",
        "'''\n",
        "\n",
        "print(\"üìù Here's the template code:\")\n",
        "print(template_code)\n",
        "\n",
        "print(\"\\nüéØ TO USE THIS TEMPLATE:\")\n",
        "print(\"1. Replace X_example and y_example with your own data\")\n",
        "print(\"2. Modify the target_name parameter\")\n",
        "print(\"3. Adjust the number of features to select (k parameter)\")\n",
        "print(\"4. Run the pipeline and analyze results\")\n",
        "\n",
        "print(\"\\nüèÅ FINAL THOUGHTS:\")\n",
        "print(\"Feature selection is both an art and a science. The best approach combines:\")\n",
        "print(\"- Domain expertise and business understanding\")\n",
        "print(\"- Statistical analysis and data-driven insights\")\n",
        "print(\"- Practical constraints and real-world considerations\")\n",
        "print(\"- Iterative testing and continuous improvement\")\n",
        "print()\n",
        "print(\"Remember: The goal is not just to build accurate models, but to build\")\n",
        "print(\"models that are interpretable, maintainable, and useful in practice!\")\n",
        "\n",
        "print(\"\\n‚ú® Congratulations! You've completed the feature selection tutorial!\")\n",
        "print(\"You now have the knowledge and tools to select features effectively for your AI projects.\")"
      ]
    }
  ]
}