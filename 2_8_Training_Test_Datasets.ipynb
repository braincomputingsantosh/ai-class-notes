{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvhZqvoJDNda"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Training and Test Datasets: Practical Examples\n",
        "===============================================\n",
        "\n",
        "This notebook demonstrates how to properly split datasets for training and testing\n",
        "AI models, with real-world examples from healthcare and finance domains.\n",
        "\n",
        "Author: AI Education Series\n",
        "Date: 2024\n",
        "\"\"\"\n",
        "\n",
        "# ========================================\n",
        "# SECTION 1: Setup and Imports\n",
        "# ========================================\n",
        "\n",
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit, StratifiedShuffleSplit\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.datasets import make_classification\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Configure plotting\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üìö This notebook will teach you how to properly split datasets for AI training and testing.\")\n",
        "\n",
        "# ========================================\n",
        "# SECTION 2: Understanding the Basics\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 2: Understanding Dataset Splitting Basics\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a simple synthetic dataset to demonstrate concepts\n",
        "def create_sample_dataset(n_samples=1000):\n",
        "    \"\"\"\n",
        "    Create a synthetic dataset for demonstration purposes.\n",
        "    This represents a simplified version of real-world data.\n",
        "    \"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=n_samples,\n",
        "        n_features=5,\n",
        "        n_redundant=0,\n",
        "        n_informative=5,\n",
        "        n_clusters_per_class=1,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create a DataFrame with meaningful column names\n",
        "    feature_names = ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5']\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df['Target'] = y\n",
        "\n",
        "    return df\n",
        "\n",
        "# Create and display our sample dataset\n",
        "sample_data = create_sample_dataset(1000)\n",
        "print(\"üìä Sample Dataset Created:\")\n",
        "print(f\"Shape: {sample_data.shape}\")\n",
        "print(f\"Columns: {list(sample_data.columns)}\")\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(sample_data.head())\n",
        "\n",
        "# Show the target distribution\n",
        "print(f\"\\nTarget Distribution:\")\n",
        "print(sample_data['Target'].value_counts())\n",
        "print(f\"Class 0: {(sample_data['Target'] == 0).sum()} samples ({(sample_data['Target'] == 0).mean():.1%})\")\n",
        "print(f\"Class 1: {(sample_data['Target'] == 1).sum()} samples ({(sample_data['Target'] == 1).mean():.1%})\")\n",
        "\n",
        "# ========================================\n",
        "# SECTION 3: Simple Train-Test Split\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 3: Simple Train-Test Split\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Separate features and target\n",
        "X = sample_data.drop('Target', axis=1)\n",
        "y = sample_data['Target']\n",
        "\n",
        "# Perform a simple 80-20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,      # 20% for testing\n",
        "    random_state=42,    # For reproducibility\n",
        "    stratify=y          # Maintain class distribution\n",
        ")\n",
        "\n",
        "print(f\"üìà Dataset Split Results:\")\n",
        "print(f\"Original dataset size: {len(X)} samples\")\n",
        "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(X):.1%})\")\n",
        "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X):.1%})\")\n",
        "\n",
        "# Check if class distribution is maintained\n",
        "print(f\"\\nClass Distribution Check:\")\n",
        "print(f\"Original - Class 0: {(y == 0).mean():.1%}, Class 1: {(y == 1).mean():.1%}\")\n",
        "print(f\"Training - Class 0: {(y_train == 0).mean():.1%}, Class 1: {(y_train == 1).mean():.1%}\")\n",
        "print(f\"Test     - Class 0: {(y_test == 0).mean():.1%}, Class 1: {(y_test == 1).mean():.1%}\")\n",
        "\n",
        "# Visualize the split\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "# Original data\n",
        "axes[0].bar(['Class 0', 'Class 1'], [sum(y == 0), sum(y == 1)], color=['lightblue', 'lightcoral'])\n",
        "axes[0].set_title('Original Dataset')\n",
        "axes[0].set_ylabel('Number of Samples')\n",
        "\n",
        "# Training data\n",
        "axes[1].bar(['Class 0', 'Class 1'], [sum(y_train == 0), sum(y_train == 1)], color=['lightblue', 'lightcoral'])\n",
        "axes[1].set_title('Training Set')\n",
        "axes[1].set_ylabel('Number of Samples')\n",
        "\n",
        "# Test data\n",
        "axes[2].bar(['Class 0', 'Class 1'], [sum(y_test == 0), sum(y_test == 1)], color=['lightblue', 'lightcoral'])\n",
        "axes[2].set_title('Test Set')\n",
        "axes[2].set_ylabel('Number of Samples')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# SECTION 4: Healthcare Example - Medical Diagnosis\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 4: Healthcare Example - Medical Diagnosis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a synthetic medical dataset\n",
        "def create_medical_dataset(n_patients=2000):\n",
        "    \"\"\"\n",
        "    Create a synthetic medical dataset for disease diagnosis.\n",
        "    This simulates patient data with various health indicators.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate patient features\n",
        "    age = np.random.normal(55, 15, n_patients)\n",
        "    age = np.clip(age, 18, 90)  # Reasonable age range\n",
        "\n",
        "    blood_pressure = np.random.normal(120, 20, n_patients)\n",
        "    cholesterol = np.random.normal(200, 40, n_patients)\n",
        "    blood_sugar = np.random.normal(100, 20, n_patients)\n",
        "    heart_rate = np.random.normal(72, 12, n_patients)\n",
        "\n",
        "    # Create target variable (disease) based on realistic relationships\n",
        "    # Higher age, BP, cholesterol, and blood sugar increase disease risk\n",
        "    disease_risk = (\n",
        "        (age - 18) / 72 * 0.3 +  # Age factor\n",
        "        np.maximum(blood_pressure - 120, 0) / 60 * 0.25 +  # BP factor\n",
        "        np.maximum(cholesterol - 200, 0) / 100 * 0.25 +  # Cholesterol factor\n",
        "        np.maximum(blood_sugar - 100, 0) / 50 * 0.2  # Blood sugar factor\n",
        "    )\n",
        "\n",
        "    # Add some randomness\n",
        "    disease_risk += np.random.normal(0, 0.1, n_patients)\n",
        "\n",
        "    # Convert to binary classification (disease: 1, no disease: 0)\n",
        "    disease = (disease_risk > 0.5).astype(int)\n",
        "\n",
        "    # Create DataFrame\n",
        "    medical_data = pd.DataFrame({\n",
        "        'Age': age,\n",
        "        'Blood_Pressure': blood_pressure,\n",
        "        'Cholesterol': cholesterol,\n",
        "        'Blood_Sugar': blood_sugar,\n",
        "        'Heart_Rate': heart_rate,\n",
        "        'Disease': disease\n",
        "    })\n",
        "\n",
        "    return medical_data\n",
        "\n",
        "# Create the medical dataset\n",
        "medical_data = create_medical_dataset(2000)\n",
        "print(\"üè• Medical Dataset Created:\")\n",
        "print(f\"Shape: {medical_data.shape}\")\n",
        "print(f\"Disease prevalence: {medical_data['Disease'].mean():.1%}\")\n",
        "print(\"\\nFirst 5 patients:\")\n",
        "print(medical_data.head())\n",
        "\n",
        "# Visualize the medical data\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "features = ['Age', 'Blood_Pressure', 'Cholesterol', 'Blood_Sugar', 'Heart_Rate']\n",
        "for i, feature in enumerate(features):\n",
        "    for disease_status in [0, 1]:\n",
        "        data = medical_data[medical_data['Disease'] == disease_status][feature]\n",
        "        axes[i].hist(data, alpha=0.7, label=f'Disease: {disease_status}', bins=30)\n",
        "    axes[i].set_title(f'{feature} Distribution')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].legend()\n",
        "\n",
        "# Disease distribution\n",
        "axes[5].bar(['No Disease', 'Disease'],\n",
        "           [sum(medical_data['Disease'] == 0), sum(medical_data['Disease'] == 1)],\n",
        "           color=['lightgreen', 'lightcoral'])\n",
        "axes[5].set_title('Disease Distribution')\n",
        "axes[5].set_ylabel('Number of Patients')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Split the medical data properly\n",
        "X_medical = medical_data.drop('Disease', axis=1)\n",
        "y_medical = medical_data['Disease']\n",
        "\n",
        "# Use stratified split to maintain disease prevalence\n",
        "X_train_med, X_test_med, y_train_med, y_test_med = train_test_split(\n",
        "    X_medical, y_medical,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_medical  # Critical for medical data!\n",
        ")\n",
        "\n",
        "print(f\"\\nüè• Medical Data Split Results:\")\n",
        "print(f\"Training set: {len(X_train_med)} patients\")\n",
        "print(f\"Test set: {len(X_test_med)} patients\")\n",
        "print(f\"Disease prevalence in training: {y_train_med.mean():.1%}\")\n",
        "print(f\"Disease prevalence in test: {y_test_med.mean():.1%}\")\n",
        "\n",
        "# Train a simple model on medical data\n",
        "# Scale the features (important for medical data)\n",
        "scaler_med = StandardScaler()\n",
        "X_train_med_scaled = scaler_med.fit_transform(X_train_med)\n",
        "X_test_med_scaled = scaler_med.transform(X_test_med)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_medical = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_medical.fit(X_train_med_scaled, y_train_med)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_med = rf_medical.predict(X_test_med_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_med = accuracy_score(y_test_med, y_pred_med)\n",
        "print(f\"\\nüìä Medical Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_med:.3f}\")\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(y_test_med, y_pred_med, target_names=['No Disease', 'Disease']))\n",
        "\n",
        "# ========================================\n",
        "# SECTION 5: Finance Example - Credit Risk Assessment\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 5: Finance Example - Credit Risk Assessment\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a synthetic financial dataset\n",
        "def create_credit_dataset(n_customers=5000):\n",
        "    \"\"\"\n",
        "    Create a synthetic credit risk dataset.\n",
        "    This simulates customer data for loan approval decisions.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate customer features\n",
        "    age = np.random.normal(35, 12, n_customers)\n",
        "    age = np.clip(age, 18, 80)\n",
        "\n",
        "    income = np.random.lognormal(10.5, 0.5, n_customers)  # Log-normal for realistic income distribution\n",
        "    income = np.clip(income, 20000, 200000)\n",
        "\n",
        "    credit_score = np.random.normal(650, 100, n_customers)\n",
        "    credit_score = np.clip(credit_score, 300, 850)\n",
        "\n",
        "    employment_years = np.random.exponential(5, n_customers)\n",
        "    employment_years = np.clip(employment_years, 0, 40)\n",
        "\n",
        "    debt_to_income = np.random.beta(2, 5, n_customers)  # Beta distribution for ratios\n",
        "\n",
        "    # Create default risk based on realistic relationships\n",
        "    default_risk = (\n",
        "        (850 - credit_score) / 550 * 0.4 +  # Lower credit score = higher risk\n",
        "        debt_to_income * 0.3 +  # Higher debt-to-income = higher risk\n",
        "        (40 - age) / 22 * 0.2 +  # Younger age = slightly higher risk\n",
        "        (10 - employment_years) / 10 * 0.1  # Less employment = higher risk\n",
        "    )\n",
        "\n",
        "    # Add some randomness\n",
        "    default_risk += np.random.normal(0, 0.1, n_customers)\n",
        "    default_risk = np.clip(default_risk, 0, 1)\n",
        "\n",
        "    # Convert to binary classification (default: 1, no default: 0)\n",
        "    # Make defaults relatively rare (realistic for good lending)\n",
        "    default = (default_risk > 0.7).astype(int)\n",
        "\n",
        "    # Create DataFrame\n",
        "    credit_data = pd.DataFrame({\n",
        "        'Age': age,\n",
        "        'Income': income,\n",
        "        'Credit_Score': credit_score,\n",
        "        'Employment_Years': employment_years,\n",
        "        'Debt_to_Income_Ratio': debt_to_income,\n",
        "        'Default': default\n",
        "    })\n",
        "\n",
        "    return credit_data\n",
        "\n",
        "# Create the credit dataset\n",
        "credit_data = create_credit_dataset(5000)\n",
        "print(\"üí≥ Credit Dataset Created:\")\n",
        "print(f\"Shape: {credit_data.shape}\")\n",
        "print(f\"Default rate: {credit_data['Default'].mean():.1%}\")\n",
        "print(\"\\nFirst 5 customers:\")\n",
        "print(credit_data.head())\n",
        "\n",
        "# Visualize the credit data\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "features = ['Age', 'Income', 'Credit_Score', 'Employment_Years', 'Debt_to_Income_Ratio']\n",
        "for i, feature in enumerate(features):\n",
        "    for default_status in [0, 1]:\n",
        "        data = credit_data[credit_data['Default'] == default_status][feature]\n",
        "        axes[i].hist(data, alpha=0.7, label=f'Default: {default_status}', bins=30)\n",
        "    axes[i].set_title(f'{feature} Distribution')\n",
        "    axes[i].set_xlabel(feature)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].legend()\n",
        "\n",
        "# Default distribution\n",
        "axes[5].bar(['No Default', 'Default'],\n",
        "           [sum(credit_data['Default'] == 0), sum(credit_data['Default'] == 1)],\n",
        "           color=['lightgreen', 'lightcoral'])\n",
        "axes[5].set_title('Default Distribution')\n",
        "axes[5].set_ylabel('Number of Customers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Split the credit data\n",
        "X_credit = credit_data.drop('Default', axis=1)\n",
        "y_credit = credit_data['Default']\n",
        "\n",
        "# Use stratified split to maintain default rate\n",
        "X_train_credit, X_test_credit, y_train_credit, y_test_credit = train_test_split(\n",
        "    X_credit, y_credit,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_credit  # Critical for imbalanced financial data!\n",
        ")\n",
        "\n",
        "print(f\"\\nüí≥ Credit Data Split Results:\")\n",
        "print(f\"Training set: {len(X_train_credit)} customers\")\n",
        "print(f\"Test set: {len(X_test_credit)} customers\")\n",
        "print(f\"Default rate in training: {y_train_credit.mean():.1%}\")\n",
        "print(f\"Default rate in test: {y_test_credit.mean():.1%}\")\n",
        "\n",
        "# Train a model on credit data\n",
        "# Scale the features\n",
        "scaler_credit = StandardScaler()\n",
        "X_train_credit_scaled = scaler_credit.fit_transform(X_train_credit)\n",
        "X_test_credit_scaled = scaler_credit.transform(X_test_credit)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_credit = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_credit.fit(X_train_credit_scaled, y_train_credit)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_credit = rf_credit.predict(X_test_credit_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_credit = accuracy_score(y_test_credit, y_pred_credit)\n",
        "print(f\"\\nüìä Credit Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_credit:.3f}\")\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(y_test_credit, y_pred_credit, target_names=['No Default', 'Default']))\n",
        "\n",
        "# ========================================\n",
        "# SECTION 6: Time Series Splitting (Financial Data)\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 6: Time Series Splitting for Financial Data\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create a time series dataset (stock prices)\n",
        "def create_stock_data(n_days=1000):\n",
        "    \"\"\"\n",
        "    Create synthetic stock price data with time series structure.\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Generate dates\n",
        "    dates = pd.date_range('2020-01-01', periods=n_days, freq='D')\n",
        "\n",
        "    # Generate stock price features\n",
        "    # Simple random walk for stock prices\n",
        "    returns = np.random.normal(0.001, 0.02, n_days)  # Daily returns\n",
        "    prices = 100 * np.exp(np.cumsum(returns))  # Stock prices\n",
        "\n",
        "    # Technical indicators\n",
        "    volume = np.random.lognormal(15, 0.5, n_days)\n",
        "    moving_avg_5 = pd.Series(prices).rolling(5).mean()\n",
        "    moving_avg_20 = pd.Series(prices).rolling(20).mean()\n",
        "\n",
        "    # Target: next day price movement (up or down)\n",
        "    next_day_return = np.roll(returns, -1)\n",
        "    target = (next_day_return > 0).astype(int)\n",
        "\n",
        "    # Create DataFrame\n",
        "    stock_data = pd.DataFrame({\n",
        "        'Date': dates,\n",
        "        'Price': prices,\n",
        "        'Volume': volume,\n",
        "        'MA_5': moving_avg_5,\n",
        "        'MA_20': moving_avg_20,\n",
        "        'Target': target\n",
        "    })\n",
        "\n",
        "    # Drop last row (no future data for target)\n",
        "    stock_data = stock_data[:-1]\n",
        "\n",
        "    # Drop rows with NaN values (from moving averages)\n",
        "    stock_data = stock_data.dropna()\n",
        "\n",
        "    return stock_data\n",
        "\n",
        "# Create stock data\n",
        "stock_data = create_stock_data(1000)\n",
        "print(\"üìà Stock Data Created:\")\n",
        "print(f\"Shape: {stock_data.shape}\")\n",
        "print(f\"Date range: {stock_data['Date'].min()} to {stock_data['Date'].max()}\")\n",
        "print(f\"Up days: {stock_data['Target'].mean():.1%}\")\n",
        "\n",
        "# Visualize stock data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Stock price over time\n",
        "axes[0, 0].plot(stock_data['Date'], stock_data['Price'])\n",
        "axes[0, 0].set_title('Stock Price Over Time')\n",
        "axes[0, 0].set_xlabel('Date')\n",
        "axes[0, 0].set_ylabel('Price ($)')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Volume over time\n",
        "axes[0, 1].plot(stock_data['Date'], stock_data['Volume'])\n",
        "axes[0, 1].set_title('Trading Volume Over Time')\n",
        "axes[0, 1].set_xlabel('Date')\n",
        "axes[0, 1].set_ylabel('Volume')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Moving averages\n",
        "axes[1, 0].plot(stock_data['Date'], stock_data['Price'], label='Price', alpha=0.7)\n",
        "axes[1, 0].plot(stock_data['Date'], stock_data['MA_5'], label='5-day MA', alpha=0.8)\n",
        "axes[1, 0].plot(stock_data['Date'], stock_data['MA_20'], label='20-day MA', alpha=0.8)\n",
        "axes[1, 0].set_title('Price and Moving Averages')\n",
        "axes[1, 0].set_xlabel('Date')\n",
        "axes[1, 0].set_ylabel('Price ($)')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Target distribution\n",
        "axes[1, 1].bar(['Down Days', 'Up Days'],\n",
        "              [sum(stock_data['Target'] == 0), sum(stock_data['Target'] == 1)],\n",
        "              color=['lightcoral', 'lightgreen'])\n",
        "axes[1, 1].set_title('Daily Price Movement Distribution')\n",
        "axes[1, 1].set_ylabel('Number of Days')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Time series splitting (CRITICAL for financial data!)\n",
        "# Never use random splitting for time series data!\n",
        "\n",
        "# Prepare features for time series model\n",
        "X_stock = stock_data[['Price', 'Volume', 'MA_5', 'MA_20']]\n",
        "y_stock = stock_data['Target']\n",
        "\n",
        "# Time-based split: 80% for training, 20% for testing\n",
        "split_point = int(0.8 * len(stock_data))\n",
        "\n",
        "X_train_stock = X_stock[:split_point]\n",
        "X_test_stock = X_stock[split_point:]\n",
        "y_train_stock = y_stock[:split_point]\n",
        "y_test_stock = y_stock[split_point:]\n",
        "\n",
        "print(f\"\\nüìà Time Series Split Results:\")\n",
        "print(f\"Training period: {stock_data['Date'].iloc[0]} to {stock_data['Date'].iloc[split_point-1]}\")\n",
        "print(f\"Testing period: {stock_data['Date'].iloc[split_point]} to {stock_data['Date'].iloc[-1]}\")\n",
        "print(f\"Training samples: {len(X_train_stock)}\")\n",
        "print(f\"Test samples: {len(X_test_stock)}\")\n",
        "\n",
        "# Visualize the time series split\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(stock_data['Date'][:split_point], stock_data['Price'][:split_point],\n",
        "         label='Training Data', color='blue', alpha=0.7)\n",
        "plt.plot(stock_data['Date'][split_point:], stock_data['Price'][split_point:],\n",
        "         label='Test Data', color='red', alpha=0.7)\n",
        "plt.axvline(x=stock_data['Date'].iloc[split_point], color='black', linestyle='--',\n",
        "            label='Split Point')\n",
        "plt.title('Time Series Split: Training vs Test Data')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Stock Price ($)')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Train model on time series data\n",
        "# Scale the features\n",
        "scaler_stock = StandardScaler()\n",
        "X_train_stock_scaled = scaler_stock.fit_transform(X_train_stock)\n",
        "X_test_stock_scaled = scaler_stock.transform(X_test_stock)\n",
        "\n",
        "# Train a Random Forest classifier\n",
        "rf_stock = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_stock.fit(X_train_stock_scaled, y_train_stock)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stock = rf_stock.predict(X_test_stock_scaled)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_stock = accuracy_score(y_test_stock, y_pred_stock)\n",
        "print(f\"\\nüìä Stock Prediction Model Performance:\")\n",
        "print(f\"Accuracy: {accuracy_stock:.3f}\")\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(y_test_stock, y_pred_stock, target_names=['Down Day', 'Up Day']))\n",
        "\n",
        "# ========================================\n",
        "# SECTION 7: Cross-Validation Example\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 7: Cross-Validation for Robust Evaluation\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# Use our medical dataset for cross-validation example\n",
        "print(\"üè• Performing Cross-Validation on Medical Data:\")\n",
        "\n",
        "# Create stratified k-fold cross-validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(rf_medical, X_train_med_scaled, y_train_med,\n",
        "                           cv=skf, scoring='accuracy')\n",
        "\n",
        "print(f\"Cross-Validation Results:\")\n",
        "print(f\"Individual fold scores: {cv_scores}\")\n",
        "print(f\"Mean CV accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
        "\n",
        "# Compare with simple train-test split\n",
        "simple_score = rf_medical.score(X_test_med_scaled, y_test_med)\n",
        "print(f\"Simple train-test accuracy: {simple_score:.3f}\")\n",
        "\n",
        "# Visualize cross-validation results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(range(1, 6), cv_scores, alpha=0.7, color='skyblue')\n",
        "plt.axhline(y=cv_scores.mean(), color='red', linestyle='--',\n",
        "            label=f'Mean: {cv_scores.mean():.3f}')\n",
        "plt.axhline(y=simple_score, color='green', linestyle='--',\n",
        "            label=f'Simple Split: {simple_score:.3f}')\n",
        "plt.title('Cross-Validation Scores vs Simple Train-Test Split')\n",
        "plt.xlabel('Fold')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n",
        "\n",
        "# ========================================\n",
        "# SECTION 8: Common Pitfalls and How to Avoid Them\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 8: Common Pitfalls and How to Avoid Them\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Pitfall 1: Data Leakage Example\n",
        "print(\"‚ö†Ô∏è PITFALL 1: Data Leakage\")\n",
        "print(\"Example: Scaling before splitting (WRONG way)\")\n",
        "\n",
        "# WRONG: Scale entire dataset before splitting\n",
        "X_wrong = sample_data.drop('Target', axis=1)\n",
        "y_wrong = sample_data['Target']\n",
        "\n",
        "# This is WRONG - it leaks information from test set to training\n",
        "scaler_wrong = StandardScaler()\n",
        "X_scaled_wrong = scaler_wrong.fit_transform(X_wrong)  # Uses ALL data for scaling\n",
        "\n",
        "# Then split\n",
        "X_train_wrong, X_test_wrong, y_train_wrong, y_test_wrong = train_test_split(\n",
        "    X_scaled_wrong, y_wrong, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"‚ùå WRONG: Scaled entire dataset before splitting\")\n",
        "print(f\"Training set mean: {X_train_wrong.mean():.4f}\")\n",
        "print(f\"Test set mean: {X_test_wrong.mean():.4f}\")\n",
        "\n",
        "# CORRECT: Scale only training data, then apply to test\n",
        "print(\"\\n‚úÖ CORRECT: Scale only training data\")\n",
        "X_train_correct, X_test_correct, y_train_correct, y_test_correct = train_test_split(\n",
        "    X_wrong, y_wrong, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_correct = StandardScaler()\n",
        "X_train_scaled_correct = scaler_correct.fit_transform(X_train_correct)  # Fit only on training\n",
        "X_test_scaled_correct = scaler_correct.transform(X_test_correct)  # Transform test using training stats\n",
        "\n",
        "print(f\"Training set mean: {X_train_scaled_correct.mean():.4f}\")\n",
        "print(f\"Test set mean: {X_test_scaled_correct.mean():.4f}\")\n",
        "\n",
        "# Pitfall 2: Ignoring Class Imbalance\n",
        "print(\"\\n‚ö†Ô∏è PITFALL 2: Ignoring Class Imbalance\")\n",
        "\n",
        "# Create imbalanced dataset\n",
        "imbalanced_data = create_medical_dataset(1000)\n",
        "# Make disease very rare (like in real life)\n",
        "imbalanced_data['Disease'] = (imbalanced_data['Disease'] == 1) & (np.random.random(1000) < 0.05)\n",
        "\n",
        "print(f\"Imbalanced dataset - Disease prevalence: {imbalanced_data['Disease'].mean():.1%}\")\n",
        "\n",
        "X_imb = imbalanced_data.drop('Disease', axis=1)\n",
        "y_imb = imbalanced_data['Disease']\n",
        "\n",
        "# WRONG: Random split without stratification\n",
        "X_train_imb_wrong, X_test_imb_wrong, y_train_imb_wrong, y_test_imb_wrong = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"‚ùå WRONG (no stratification):\")\n",
        "print(f\"Training disease rate: {y_train_imb_wrong.mean():.1%}\")\n",
        "print(f\"Test disease rate: {y_test_imb_wrong.mean():.1%}\")\n",
        "\n",
        "# CORRECT: Use stratified split\n",
        "X_train_imb_correct, X_test_imb_correct, y_train_imb_correct, y_test_imb_correct = train_test_split(\n",
        "    X_imb, y_imb, test_size=0.2, random_state=42, stratify=y_imb\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ CORRECT (with stratification):\")\n",
        "print(f\"Training disease rate: {y_train_imb_correct.mean():.1%}\")\n",
        "print(f\"Test disease rate: {y_test_imb_correct.mean():.1%}\")\n",
        "\n",
        "# Pitfall 3: Wrong splitting for time series\n",
        "print(\"\\n‚ö†Ô∏è PITFALL 3: Random Splitting for Time Series\")\n",
        "\n",
        "# Show why random splitting fails for time series\n",
        "dates = pd.date_range('2020-01-01', periods=100, freq='D')\n",
        "time_series_data = pd.DataFrame({\n",
        "    'Date': dates,\n",
        "    'Value': np.cumsum(np.random.randn(100)) + 100  # Random walk\n",
        "})\n",
        "\n",
        "# WRONG: Random split\n",
        "indices = np.arange(len(time_series_data))\n",
        "train_idx_wrong, test_idx_wrong = train_test_split(indices, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"‚ùå WRONG: Random split for time series\")\n",
        "print(f\"Training dates range: {time_series_data.iloc[train_idx_wrong]['Date'].min()} to {time_series_data.iloc[train_idx_wrong]['Date'].max()}\")\n",
        "print(f\"Test dates range: {time_series_data.iloc[test_idx_wrong]['Date'].min()} to {time_series_data.iloc[test_idx_wrong]['Date'].max()}\")\n",
        "\n",
        "# CORRECT: Temporal split\n",
        "split_point = int(0.8 * len(time_series_data))\n",
        "train_idx_correct = np.arange(split_point)\n",
        "test_idx_correct = np.arange(split_point, len(time_series_data))\n",
        "\n",
        "print(f\"‚úÖ CORRECT: Temporal split\")\n",
        "print(f\"Training dates range: {time_series_data.iloc[train_idx_correct]['Date'].min()} to {time_series_data.iloc[train_idx_correct]['Date'].max()}\")\n",
        "print(f\"Test dates range: {time_series_data.iloc[test_idx_correct]['Date'].min()} to {time_series_data.iloc[test_idx_correct]['Date'].max()}\")\n",
        "\n",
        "# ========================================\n",
        "# SECTION 9: Best Practices Summary\n",
        "# ========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SECTION 9: Best Practices Summary\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"‚úÖ BEST PRACTICES CHECKLIST:\")\n",
        "print(\"1. ‚úì Always maintain strict separation between train and test sets\")\n",
        "print(\"2. ‚úì Use stratified splitting for imbalanced datasets\")\n",
        "print(\"3. ‚úì Use temporal splitting for time series data\")\n",
        "print(\"4. ‚úì Scale/normalize data AFTER splitting, not before\")\n",
        "print(\"5. ‚úì Ensure test set represents real-world conditions\")\n",
        "print(\"6. ‚úì Use cross-validation for robust performance estimates\")\n",
        "print(\"7. ‚úì Check for data leakage and temporal dependencies\")\n",
        "print(\"8. ‚úì Document your splitting strategy and rationale\")\n",
        "print(\"9. ‚úì Monitor for distribution shifts between train and test\")\n",
        "print(\"10. ‚úì Reserve test set for final evaluation only\")\n",
        "\n",
        "print(\"\\n‚ùå COMMON MISTAKES TO AVOID:\")\n",
        "print(\"1. ‚úó Using test set for model selection or hyperparameter tuning\")\n",
        "print(\"2. ‚úó Preprocessing entire dataset before splitting\")\n",
        "print(\"3. ‚úó Random splitting for time series or sequential data\")\n",
        "print(\"4. ‚úó Ignoring class imbalance in splitting\")\n",
        "print(\"5. ‚úó Including duplicate or highly similar samples in both sets\")\n",
        "print(\"6. ‚úó Not checking if test set is representative\")\n",
        "print(\"7. ‚úó Using too small test sets (less than 15-20%)\")\n",
        "print(\"8. ‚úó Not documenting the splitting process\")\n",
        "\n",
        "print(\"\\nüéØ DOMAIN-SPECIFIC CONSIDERATIONS:\")\n",
        "print(\"Healthcare:\")\n",
        "print(\"- Ensure patient data doesn't leak between sets\")\n",
        "print(\"- Consider temporal aspects of medical data\")\n",
        "print(\"- Account for rare diseases in stratification\")\n",
        "print(\"- Validate across different hospitals/demographics\")\n",
        "\n",
        "print(\"\\nFinance:\")\n",
        "print(\"- Always use temporal splitting for market data\")\n",
        "print(\"- Account for economic regime changes\")\n",
        "print(\"- Consider transaction-level vs customer-level splitting\")\n",
        "print(\"- Be aware of survivorship bias in historical data\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ CONGRATULATIONS!\")\n",
        "print(\"You've completed the comprehensive guide to training and test datasets!\")\n",
        "print(\"Remember: Proper dataset splitting is the foundation of trustworthy AI.\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Final demonstration: Quick model comparison\n",
        "print(\"\\nüìä FINAL DEMONSTRATION: Model Comparison\")\n",
        "\n",
        "# Compare performance across our three domains\n",
        "models_performance = {\n",
        "    'Medical Diagnosis': accuracy_med,\n",
        "    'Credit Risk': accuracy_credit,\n",
        "    'Stock Prediction': accuracy_stock\n",
        "}\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "domains = list(models_performance.keys())\n",
        "accuracies = list(models_performance.values())\n",
        "\n",
        "bars = plt.bar(domains, accuracies, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
        "plt.title('Model Performance Across Different Domains')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, acc in zip(bars, accuracies):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "             f'{acc:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nModel Performance Summary:\")\n",
        "for domain, accuracy in models_performance.items():\n",
        "    print(f\"{domain}: {accuracy:.3f}\")\n",
        "\n",
        "print(f\"\\nüöÄ Next Steps:\")\n",
        "print(\"1. Practice with your own datasets\")\n",
        "print(\"2. Experiment with different splitting strategies\")\n",
        "print(\"3. Try cross-validation on various problems\")\n",
        "print(\"4. Always validate your splits make sense for your domain\")\n",
        "print(\"5. Remember: Good data splitting is the key to reliable AI!\")"
      ]
    }
  ]
}