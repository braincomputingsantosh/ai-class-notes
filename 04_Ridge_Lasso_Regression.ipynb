{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffhT2cCV_rso"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"=== Ridge and Lasso Regression Demonstration ===\\n\")\n",
        "\n",
        "# 1. Generate synthetic dataset with correlated features\n",
        "print(\"1. Generating synthetic dataset...\")\n",
        "# Create base dataset\n",
        "X, y = make_regression(n_samples=100, n_features=20, noise=10, random_state=42)\n",
        "\n",
        "# Add some highly correlated features to demonstrate multicollinearity\n",
        "X_corr = np.column_stack([\n",
        "    X,\n",
        "    X[:, 0] + np.random.normal(0, 0.1, X.shape[0]),  # Highly correlated with feature 0\n",
        "    X[:, 1] + np.random.normal(0, 0.1, X.shape[0]),  # Highly correlated with feature 1\n",
        "    X[:, 2] * 2 + np.random.normal(0, 0.2, X.shape[0])  # Scaled version of feature 2\n",
        "])\n",
        "\n",
        "print(f\"Dataset shape: {X_corr.shape}\")\n",
        "print(f\"Number of samples: {X_corr.shape[0]}\")\n",
        "print(f\"Number of features: {X_corr.shape[1]}\")\n",
        "\n",
        "# 2. Split the data\n",
        "print(\"\\n2. Splitting data into train and test sets...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_corr, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "\n",
        "# 3. Standardize the data (crucial for regularization!)\n",
        "print(\"\\n3. Standardizing features...\")\n",
        "print(\"Note: Standardization is crucial for Ridge and Lasso because:\")\n",
        "print(\"- Regularization penalties are applied equally to all coefficients\")\n",
        "print(\"- Features with larger scales would be penalized less without standardization\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 4. Train standard Linear Regression for comparison\n",
        "print(\"\\n4. Training Standard Linear Regression...\")\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
        "r2_lr = r2_score(y_test, y_pred_lr)\n",
        "\n",
        "print(f\"Linear Regression - Test MSE: {mse_lr:.2f}, R²: {r2_lr:.3f}\")\n",
        "\n",
        "# 5. Train Ridge Regression with different alpha values\n",
        "print(\"\\n5. Training Ridge Regression...\")\n",
        "alphas_ridge = [0.1, 1.0, 10.0, 100.0]\n",
        "ridge_results = {}\n",
        "\n",
        "for alpha in alphas_ridge:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    ridge.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred_ridge = ridge.predict(X_test_scaled)\n",
        "    mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
        "    r2_ridge = r2_score(y_test, y_pred_ridge)\n",
        "\n",
        "    ridge_results[alpha] = {\n",
        "        'model': ridge,\n",
        "        'mse': mse_ridge,\n",
        "        'r2': r2_ridge,\n",
        "        'coefficients': ridge.coef_\n",
        "    }\n",
        "\n",
        "    print(f\"Ridge (α={alpha}) - Test MSE: {mse_ridge:.2f}, R²: {r2_ridge:.3f}\")\n",
        "\n",
        "# 6. Train Lasso Regression with different alpha values\n",
        "print(\"\\n6. Training Lasso Regression...\")\n",
        "alphas_lasso = [0.1, 1.0, 10.0, 50.0]\n",
        "lasso_results = {}\n",
        "\n",
        "for alpha in alphas_lasso:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=2000)\n",
        "    lasso.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_pred_lasso = lasso.predict(X_test_scaled)\n",
        "    mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
        "    r2_lasso = r2_score(y_test, y_pred_lasso)\n",
        "\n",
        "    # Count non-zero coefficients\n",
        "    non_zero_coef = np.sum(np.abs(lasso.coef_) > 1e-5)\n",
        "\n",
        "    lasso_results[alpha] = {\n",
        "        'model': lasso,\n",
        "        'mse': mse_lasso,\n",
        "        'r2': r2_lasso,\n",
        "        'coefficients': lasso.coef_,\n",
        "        'non_zero_features': non_zero_coef\n",
        "    }\n",
        "\n",
        "    print(f\"Lasso (α={alpha}) - Test MSE: {mse_lasso:.2f}, R²: {r2_lasso:.3f}, \"\n",
        "          f\"Non-zero features: {non_zero_coef}/{X_train.shape[1]}\")\n",
        "\n",
        "# 7. Visualize coefficient magnitudes\n",
        "print(\"\\n7. Creating visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Plot 1: Coefficient comparison for different models\n",
        "ax1 = axes[0, 0]\n",
        "feature_indices = range(len(lr.coef_))\n",
        "\n",
        "ax1.scatter(feature_indices, lr.coef_, alpha=0.7, label='Linear Regression', s=50)\n",
        "ax1.scatter(feature_indices, ridge_results[1.0]['coefficients'],\n",
        "           alpha=0.7, label='Ridge (α=1.0)', s=50)\n",
        "ax1.scatter(feature_indices, lasso_results[1.0]['coefficients'],\n",
        "           alpha=0.7, label='Lasso (α=1.0)', s=50)\n",
        "\n",
        "ax1.set_xlabel('Feature Index')\n",
        "ax1.set_ylabel('Coefficient Value')\n",
        "ax1.set_title('Coefficient Comparison: Linear vs Ridge vs Lasso')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: Ridge coefficients vs alpha\n",
        "ax2 = axes[0, 1]\n",
        "for i in range(min(10, X_train.shape[1])):  # Show first 10 features\n",
        "    ridge_coefs = [ridge_results[alpha]['coefficients'][i] for alpha in alphas_ridge]\n",
        "    ax2.plot(alphas_ridge, ridge_coefs, 'o-', alpha=0.7, label=f'Feature {i}')\n",
        "\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_xlabel('Regularization Parameter (α)')\n",
        "ax2.set_ylabel('Coefficient Value')\n",
        "ax2.set_title('Ridge: Coefficient Shrinkage vs α')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Plot 3: Lasso coefficients vs alpha\n",
        "ax3 = axes[1, 0]\n",
        "for i in range(min(10, X_train.shape[1])):  # Show first 10 features\n",
        "    lasso_coefs = [lasso_results[alpha]['coefficients'][i] for alpha in alphas_lasso]\n",
        "    ax3.plot(alphas_lasso, lasso_coefs, 'o-', alpha=0.7, label=f'Feature {i}')\n",
        "\n",
        "ax3.set_xscale('log')\n",
        "ax3.set_xlabel('Regularization Parameter (α)')\n",
        "ax3.set_ylabel('Coefficient Value')\n",
        "ax3.set_title('Lasso: Coefficient Shrinkage vs α')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Plot 4: Model performance comparison\n",
        "ax4 = axes[1, 1]\n",
        "models = ['Linear Reg'] + [f'Ridge α={a}' for a in alphas_ridge] + [f'Lasso α={a}' for a in alphas_lasso]\n",
        "mse_values = [mse_lr] + [ridge_results[a]['mse'] for a in alphas_ridge] + [lasso_results[a]['mse'] for a in alphas_lasso]\n",
        "\n",
        "colors = ['blue'] + ['green'] * len(alphas_ridge) + ['red'] * len(alphas_lasso)\n",
        "bars = ax4.bar(range(len(models)), mse_values, color=colors, alpha=0.7)\n",
        "\n",
        "ax4.set_xlabel('Model')\n",
        "ax4.set_ylabel('Test MSE')\n",
        "ax4.set_title('Model Performance Comparison')\n",
        "ax4.set_xticks(range(len(models)))\n",
        "ax4.set_xticklabels(models, rotation=45, ha='right')\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, mse in zip(bars, mse_values):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
        "             f'{mse:.1f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 8. Cross-validation for optimal alpha selection\n",
        "print(\"\\n8. Finding optimal alpha using cross-validation...\")\n",
        "\n",
        "# Ridge cross-validation\n",
        "alpha_range = np.logspace(-2, 2, 20)  # From 0.01 to 100\n",
        "ridge_cv_scores = []\n",
        "\n",
        "for alpha in alpha_range:\n",
        "    ridge = Ridge(alpha=alpha)\n",
        "    scores = cross_val_score(ridge, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    ridge_cv_scores.append(-scores.mean())\n",
        "\n",
        "best_ridge_alpha = alpha_range[np.argmin(ridge_cv_scores)]\n",
        "\n",
        "# Lasso cross-validation\n",
        "lasso_cv_scores = []\n",
        "\n",
        "for alpha in alpha_range:\n",
        "    lasso = Lasso(alpha=alpha, max_iter=2000)\n",
        "    scores = cross_val_score(lasso, X_train_scaled, y_train, cv=5, scoring='neg_mean_squared_error')\n",
        "    lasso_cv_scores.append(-scores.mean())\n",
        "\n",
        "best_lasso_alpha = alpha_range[np.argmin(lasso_cv_scores)]\n",
        "\n",
        "print(f\"Best Ridge alpha: {best_ridge_alpha:.3f}\")\n",
        "print(f\"Best Lasso alpha: {best_lasso_alpha:.3f}\")\n",
        "\n",
        "# Plot cross-validation results\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(alpha_range, ridge_cv_scores, 'g-o', alpha=0.7)\n",
        "plt.axvline(best_ridge_alpha, color='red', linestyle='--', alpha=0.7, label=f'Best α={best_ridge_alpha:.3f}')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Regularization Parameter (α)')\n",
        "plt.ylabel('CV Mean Squared Error')\n",
        "plt.title('Ridge: Cross-Validation Score vs α')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(alpha_range, lasso_cv_scores, 'r-o', alpha=0.7)\n",
        "plt.axvline(best_lasso_alpha, color='red', linestyle='--', alpha=0.7, label=f'Best α={best_lasso_alpha:.3f}')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Regularization Parameter (α)')\n",
        "plt.ylabel('CV Mean Squared Error')\n",
        "plt.title('Lasso: Cross-Validation Score vs α')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 9. Final model evaluation with optimal alpha\n",
        "print(\"\\n9. Final evaluation with optimal alpha values...\")\n",
        "\n",
        "# Train final models with best alpha\n",
        "final_ridge = Ridge(alpha=best_ridge_alpha)\n",
        "final_ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_final_ridge = final_ridge.predict(X_test_scaled)\n",
        "\n",
        "final_lasso = Lasso(alpha=best_lasso_alpha, max_iter=2000)\n",
        "final_lasso.fit(X_train_scaled, y_train)\n",
        "y_pred_final_lasso = final_lasso.predict(X_test_scaled)\n",
        "\n",
        "# Calculate final metrics\n",
        "final_ridge_mse = mean_squared_error(y_test, y_pred_final_ridge)\n",
        "final_ridge_r2 = r2_score(y_test, y_pred_final_ridge)\n",
        "\n",
        "final_lasso_mse = mean_squared_error(y_test, y_pred_final_lasso)\n",
        "final_lasso_r2 = r2_score(y_test, y_pred_final_lasso)\n",
        "final_lasso_features = np.sum(np.abs(final_lasso.coef_) > 1e-5)\n",
        "\n",
        "print(\"\\n=== FINAL RESULTS ===\")\n",
        "print(f\"Linear Regression  - MSE: {mse_lr:.2f}, R²: {r2_lr:.3f}\")\n",
        "print(f\"Ridge (α={best_ridge_alpha:.3f}) - MSE: {final_ridge_mse:.2f}, R²: {final_ridge_r2:.3f}\")\n",
        "print(f\"Lasso (α={best_lasso_alpha:.3f}) - MSE: {final_lasso_mse:.2f}, R²: {final_lasso_r2:.3f}, Features: {final_lasso_features}/{X_train.shape[1]}\")\n",
        "\n",
        "# 10. Key takeaways\n",
        "print(\"\\n=== KEY TAKEAWAYS ===\")\n",
        "print(\"1. Regularization typically improves generalization performance\")\n",
        "print(\"2. Ridge shrinks coefficients but keeps all features\")\n",
        "print(\"3. Lasso can shrink coefficients to exactly zero, performing feature selection\")\n",
        "print(\"4. Cross-validation is essential for selecting optimal regularization strength\")\n",
        "print(\"5. Feature standardization is crucial for fair regularization\")\n",
        "print(\"6. The optimal alpha balances bias and variance for best generalization\")"
      ]
    }
  ]
}